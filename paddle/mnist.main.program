{ // block 0
    var src_ids : fluid.VarType.LOD_TENSOR.shape(2, 64, 1).astype(VarType.INT64)
    var pos_ids : fluid.VarType.LOD_TENSOR.shape(2, 64, 1).astype(VarType.INT64)
    var sent_ids : fluid.VarType.LOD_TENSOR.shape(2, 64, 1).astype(VarType.INT64)
    var task_ids : fluid.VarType.LOD_TENSOR.shape(2, 64, 1).astype(VarType.INT64)
    var input_mask : fluid.VarType.LOD_TENSOR.shape(2, 64, 1).astype(VarType.FP32)
    var mask_label : fluid.VarType.LOD_TENSOR.shape(-1, 1).astype(VarType.INT64)
    var mask_pos : fluid.VarType.LOD_TENSOR.shape(-1,).astype(VarType.INT32)
    var lm_weight : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var batch_mask : fluid.VarType.LOD_TENSOR.shape(-1, 1).astype(VarType.FP32)
    var loss_mask : fluid.VarType.LOD_TENSOR.shape(-1, 1).astype(VarType.FP32)
    var gather_idx : fluid.VarType.LOD_TENSOR.shape(-1,).astype(VarType.INT32)
    var task_label_0 : fluid.VarType.LOD_TENSOR.shape(-1, 1).astype(VarType.INT64)
    var task_weight_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var task_label_1 : fluid.VarType.LOD_TENSOR.shape(-1, 1).astype(VarType.INT64)
    var task_weight_1 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var task_label_2 : fluid.VarType.LOD_TENSOR.shape(-1, 1).astype(VarType.INT64)
    var task_weight_2 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    persist trainable param word_embedding : fluid.VarType.LOD_TENSOR.shape(19000, 768).astype(VarType.FP32)
    var embedding_0.tmp_0 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    persist trainable param pos_embedding : fluid.VarType.LOD_TENSOR.shape(2048, 768).astype(VarType.FP32)
    var embedding_1.tmp_0 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    persist trainable param sent_embedding : fluid.VarType.LOD_TENSOR.shape(4, 768).astype(VarType.FP32)
    var embedding_2.tmp_0 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    persist trainable param task_embedding : fluid.VarType.LOD_TENSOR.shape(3, 768).astype(VarType.FP32)
    var embedding_3.tmp_0 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var tmp_0 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var tmp_1 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var tmp_2 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    persist trainable param pre_encoder_layer_norm_scale : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    persist trainable param pre_encoder_layer_norm_bias : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var layer_norm_0.tmp_0 : fluid.VarType.LOD_TENSOR.shape(128,).astype(VarType.FP32)
    var layer_norm_0.tmp_1 : fluid.VarType.LOD_TENSOR.shape(128,).astype(VarType.FP32)
    var layer_norm_0.tmp_2 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var matmul_0.tmp_0 : fluid.VarType.LOD_TENSOR.shape(2, 64, 64).astype(VarType.FP32)
    var scale_0.tmp_0 : fluid.VarType.LOD_TENSOR.shape(2, 64, 64).astype(VarType.FP32)
    var stack_0.tmp_0 : fluid.VarType.LOD_TENSOR.shape(2, 12, 64, 64).astype(VarType.FP32)
    persist trainable param encoder_layer_0_multi_head_att_query_fc.w_0 : fluid.VarType.LOD_TENSOR.shape(768, 768).astype(VarType.FP32)
    var fc_0.tmp_0 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    persist trainable param encoder_layer_0_multi_head_att_query_fc.b_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var fc_0.tmp_1 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    persist trainable param encoder_layer_0_multi_head_att_key_fc.w_0 : fluid.VarType.LOD_TENSOR.shape(768, 768).astype(VarType.FP32)
    var fc_1.tmp_0 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    persist trainable param encoder_layer_0_multi_head_att_key_fc.b_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var fc_1.tmp_1 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    persist trainable param encoder_layer_0_multi_head_att_value_fc.w_0 : fluid.VarType.LOD_TENSOR.shape(768, 768).astype(VarType.FP32)
    var fc_2.tmp_0 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    persist trainable param encoder_layer_0_multi_head_att_value_fc.b_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var fc_2.tmp_1 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var reshape2_0.tmp_0 : fluid.VarType.LOD_TENSOR.shape(2, 64, 12, 64).astype(VarType.FP32)
    var reshape2_0.tmp_1 : fluid.VarType.LOD_TENSOR.shape(0, 2, 64, 768).astype(VarType.FP32)
    var transpose_0.tmp_0 : fluid.VarType.LOD_TENSOR.shape(2, 12, 64, 64).astype(VarType.FP32)
    var transpose_0.tmp_1 : fluid.VarType.LOD_TENSOR.shape(0, 2, 64, 12, 64).astype(VarType.FP32)
    var reshape2_1.tmp_0 : fluid.VarType.LOD_TENSOR.shape(2, 64, 12, 64).astype(VarType.FP32)
    var reshape2_1.tmp_1 : fluid.VarType.LOD_TENSOR.shape(0, 2, 64, 768).astype(VarType.FP32)
    var transpose_1.tmp_0 : fluid.VarType.LOD_TENSOR.shape(2, 12, 64, 64).astype(VarType.FP32)
    var transpose_1.tmp_1 : fluid.VarType.LOD_TENSOR.shape(0, 2, 64, 12, 64).astype(VarType.FP32)
    var reshape2_2.tmp_0 : fluid.VarType.LOD_TENSOR.shape(2, 64, 12, 64).astype(VarType.FP32)
    var reshape2_2.tmp_1 : fluid.VarType.LOD_TENSOR.shape(0, 2, 64, 768).astype(VarType.FP32)
    var transpose_2.tmp_0 : fluid.VarType.LOD_TENSOR.shape(2, 12, 64, 64).astype(VarType.FP32)
    var transpose_2.tmp_1 : fluid.VarType.LOD_TENSOR.shape(0, 2, 64, 12, 64).astype(VarType.FP32)
    var scale_1.tmp_0 : fluid.VarType.LOD_TENSOR.shape(2, 12, 64, 64).astype(VarType.FP32)
    var matmul_1.tmp_0 : fluid.VarType.LOD_TENSOR.shape(2, 12, 64, 64).astype(VarType.FP32)
    var tmp_3 : fluid.VarType.LOD_TENSOR.shape(2, 12, 64, 64).astype(VarType.FP32)
    var softmax_0.tmp_0 : fluid.VarType.LOD_TENSOR.shape(2, 12, 64, 64).astype(VarType.FP32)
    var matmul_2.tmp_0 : fluid.VarType.LOD_TENSOR.shape(2, 12, 64, 64).astype(VarType.FP32)
    var transpose_3.tmp_0 : fluid.VarType.LOD_TENSOR.shape(2, 64, 12, 64).astype(VarType.FP32)
    var transpose_3.tmp_1 : fluid.VarType.LOD_TENSOR.shape(0, 2, 12, 64, 64).astype(VarType.FP32)
    var reshape2_3.tmp_0 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var reshape2_3.tmp_1 : fluid.VarType.LOD_TENSOR.shape(0, 2, 64, 12, 64).astype(VarType.FP32)
    persist trainable param encoder_layer_0_multi_head_att_output_fc.w_0 : fluid.VarType.LOD_TENSOR.shape(768, 768).astype(VarType.FP32)
    var fc_3.tmp_0 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    persist trainable param encoder_layer_0_multi_head_att_output_fc.b_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var fc_3.tmp_1 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var tmp_4 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    persist trainable param encoder_layer_0_post_att_layer_norm_scale : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    persist trainable param encoder_layer_0_post_att_layer_norm_bias : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var layer_norm_1.tmp_0 : fluid.VarType.LOD_TENSOR.shape(128,).astype(VarType.FP32)
    var layer_norm_1.tmp_1 : fluid.VarType.LOD_TENSOR.shape(128,).astype(VarType.FP32)
    var layer_norm_1.tmp_2 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    persist trainable param encoder_layer_0_ffn_fc_0.w_0 : fluid.VarType.LOD_TENSOR.shape(768, 3072).astype(VarType.FP32)
    var fc_4.tmp_0 : fluid.VarType.LOD_TENSOR.shape(2, 64, 3072).astype(VarType.FP32)
    persist trainable param encoder_layer_0_ffn_fc_0.b_0 : fluid.VarType.LOD_TENSOR.shape(3072,).astype(VarType.FP32)
    var fc_4.tmp_1 : fluid.VarType.LOD_TENSOR.shape(2, 64, 3072).astype(VarType.FP32)
    var fc_4.tmp_2 : fluid.VarType.LOD_TENSOR.shape(2, 64, 3072).astype(VarType.FP32)
    persist trainable param encoder_layer_0_ffn_fc_1.w_0 : fluid.VarType.LOD_TENSOR.shape(3072, 768).astype(VarType.FP32)
    var fc_5.tmp_0 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    persist trainable param encoder_layer_0_ffn_fc_1.b_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var fc_5.tmp_1 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var tmp_5 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    persist trainable param encoder_layer_0_post_ffn_layer_norm_scale : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    persist trainable param encoder_layer_0_post_ffn_layer_norm_bias : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var layer_norm_2.tmp_0 : fluid.VarType.LOD_TENSOR.shape(128,).astype(VarType.FP32)
    var layer_norm_2.tmp_1 : fluid.VarType.LOD_TENSOR.shape(128,).astype(VarType.FP32)
    var layer_norm_2.tmp_2 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    persist trainable param encoder_layer_1_multi_head_att_query_fc.w_0 : fluid.VarType.LOD_TENSOR.shape(768, 768).astype(VarType.FP32)
    var fc_6.tmp_0 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    persist trainable param encoder_layer_1_multi_head_att_query_fc.b_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var fc_6.tmp_1 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    persist trainable param encoder_layer_1_multi_head_att_key_fc.w_0 : fluid.VarType.LOD_TENSOR.shape(768, 768).astype(VarType.FP32)
    var fc_7.tmp_0 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    persist trainable param encoder_layer_1_multi_head_att_key_fc.b_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var fc_7.tmp_1 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    persist trainable param encoder_layer_1_multi_head_att_value_fc.w_0 : fluid.VarType.LOD_TENSOR.shape(768, 768).astype(VarType.FP32)
    var fc_8.tmp_0 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    persist trainable param encoder_layer_1_multi_head_att_value_fc.b_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var fc_8.tmp_1 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var reshape2_4.tmp_0 : fluid.VarType.LOD_TENSOR.shape(2, 64, 12, 64).astype(VarType.FP32)
    var reshape2_4.tmp_1 : fluid.VarType.LOD_TENSOR.shape(0, 2, 64, 768).astype(VarType.FP32)
    var transpose_4.tmp_0 : fluid.VarType.LOD_TENSOR.shape(2, 12, 64, 64).astype(VarType.FP32)
    var transpose_4.tmp_1 : fluid.VarType.LOD_TENSOR.shape(0, 2, 64, 12, 64).astype(VarType.FP32)
    var reshape2_5.tmp_0 : fluid.VarType.LOD_TENSOR.shape(2, 64, 12, 64).astype(VarType.FP32)
    var reshape2_5.tmp_1 : fluid.VarType.LOD_TENSOR.shape(0, 2, 64, 768).astype(VarType.FP32)
    var transpose_5.tmp_0 : fluid.VarType.LOD_TENSOR.shape(2, 12, 64, 64).astype(VarType.FP32)
    var transpose_5.tmp_1 : fluid.VarType.LOD_TENSOR.shape(0, 2, 64, 12, 64).astype(VarType.FP32)
    var reshape2_6.tmp_0 : fluid.VarType.LOD_TENSOR.shape(2, 64, 12, 64).astype(VarType.FP32)
    var reshape2_6.tmp_1 : fluid.VarType.LOD_TENSOR.shape(0, 2, 64, 768).astype(VarType.FP32)
    var transpose_6.tmp_0 : fluid.VarType.LOD_TENSOR.shape(2, 12, 64, 64).astype(VarType.FP32)
    var transpose_6.tmp_1 : fluid.VarType.LOD_TENSOR.shape(0, 2, 64, 12, 64).astype(VarType.FP32)
    var scale_2.tmp_0 : fluid.VarType.LOD_TENSOR.shape(2, 12, 64, 64).astype(VarType.FP32)
    var matmul_3.tmp_0 : fluid.VarType.LOD_TENSOR.shape(2, 12, 64, 64).astype(VarType.FP32)
    var tmp_6 : fluid.VarType.LOD_TENSOR.shape(2, 12, 64, 64).astype(VarType.FP32)
    var softmax_1.tmp_0 : fluid.VarType.LOD_TENSOR.shape(2, 12, 64, 64).astype(VarType.FP32)
    var matmul_4.tmp_0 : fluid.VarType.LOD_TENSOR.shape(2, 12, 64, 64).astype(VarType.FP32)
    var transpose_7.tmp_0 : fluid.VarType.LOD_TENSOR.shape(2, 64, 12, 64).astype(VarType.FP32)
    var transpose_7.tmp_1 : fluid.VarType.LOD_TENSOR.shape(0, 2, 12, 64, 64).astype(VarType.FP32)
    var reshape2_7.tmp_0 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var reshape2_7.tmp_1 : fluid.VarType.LOD_TENSOR.shape(0, 2, 64, 12, 64).astype(VarType.FP32)
    persist trainable param encoder_layer_1_multi_head_att_output_fc.w_0 : fluid.VarType.LOD_TENSOR.shape(768, 768).astype(VarType.FP32)
    var fc_9.tmp_0 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    persist trainable param encoder_layer_1_multi_head_att_output_fc.b_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var fc_9.tmp_1 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var tmp_7 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    persist trainable param encoder_layer_1_post_att_layer_norm_scale : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    persist trainable param encoder_layer_1_post_att_layer_norm_bias : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var layer_norm_3.tmp_0 : fluid.VarType.LOD_TENSOR.shape(128,).astype(VarType.FP32)
    var layer_norm_3.tmp_1 : fluid.VarType.LOD_TENSOR.shape(128,).astype(VarType.FP32)
    var layer_norm_3.tmp_2 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    persist trainable param encoder_layer_1_ffn_fc_0.w_0 : fluid.VarType.LOD_TENSOR.shape(768, 3072).astype(VarType.FP32)
    var fc_10.tmp_0 : fluid.VarType.LOD_TENSOR.shape(2, 64, 3072).astype(VarType.FP32)
    persist trainable param encoder_layer_1_ffn_fc_0.b_0 : fluid.VarType.LOD_TENSOR.shape(3072,).astype(VarType.FP32)
    var fc_10.tmp_1 : fluid.VarType.LOD_TENSOR.shape(2, 64, 3072).astype(VarType.FP32)
    var fc_10.tmp_2 : fluid.VarType.LOD_TENSOR.shape(2, 64, 3072).astype(VarType.FP32)
    persist trainable param encoder_layer_1_ffn_fc_1.w_0 : fluid.VarType.LOD_TENSOR.shape(3072, 768).astype(VarType.FP32)
    var fc_11.tmp_0 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    persist trainable param encoder_layer_1_ffn_fc_1.b_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var fc_11.tmp_1 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var tmp_8 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    persist trainable param encoder_layer_1_post_ffn_layer_norm_scale : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    persist trainable param encoder_layer_1_post_ffn_layer_norm_bias : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var layer_norm_4.tmp_0 : fluid.VarType.LOD_TENSOR.shape(128,).astype(VarType.FP32)
    var layer_norm_4.tmp_1 : fluid.VarType.LOD_TENSOR.shape(128,).astype(VarType.FP32)
    var layer_norm_4.tmp_2 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    persist trainable param encoder_layer_2_multi_head_att_query_fc.w_0 : fluid.VarType.LOD_TENSOR.shape(768, 768).astype(VarType.FP32)
    var fc_12.tmp_0 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    persist trainable param encoder_layer_2_multi_head_att_query_fc.b_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var fc_12.tmp_1 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    persist trainable param encoder_layer_2_multi_head_att_key_fc.w_0 : fluid.VarType.LOD_TENSOR.shape(768, 768).astype(VarType.FP32)
    var fc_13.tmp_0 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    persist trainable param encoder_layer_2_multi_head_att_key_fc.b_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var fc_13.tmp_1 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    persist trainable param encoder_layer_2_multi_head_att_value_fc.w_0 : fluid.VarType.LOD_TENSOR.shape(768, 768).astype(VarType.FP32)
    var fc_14.tmp_0 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    persist trainable param encoder_layer_2_multi_head_att_value_fc.b_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var fc_14.tmp_1 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var reshape2_8.tmp_0 : fluid.VarType.LOD_TENSOR.shape(2, 64, 12, 64).astype(VarType.FP32)
    var reshape2_8.tmp_1 : fluid.VarType.LOD_TENSOR.shape(0, 2, 64, 768).astype(VarType.FP32)
    var transpose_8.tmp_0 : fluid.VarType.LOD_TENSOR.shape(2, 12, 64, 64).astype(VarType.FP32)
    var transpose_8.tmp_1 : fluid.VarType.LOD_TENSOR.shape(0, 2, 64, 12, 64).astype(VarType.FP32)
    var reshape2_9.tmp_0 : fluid.VarType.LOD_TENSOR.shape(2, 64, 12, 64).astype(VarType.FP32)
    var reshape2_9.tmp_1 : fluid.VarType.LOD_TENSOR.shape(0, 2, 64, 768).astype(VarType.FP32)
    var transpose_9.tmp_0 : fluid.VarType.LOD_TENSOR.shape(2, 12, 64, 64).astype(VarType.FP32)
    var transpose_9.tmp_1 : fluid.VarType.LOD_TENSOR.shape(0, 2, 64, 12, 64).astype(VarType.FP32)
    var reshape2_10.tmp_0 : fluid.VarType.LOD_TENSOR.shape(2, 64, 12, 64).astype(VarType.FP32)
    var reshape2_10.tmp_1 : fluid.VarType.LOD_TENSOR.shape(0, 2, 64, 768).astype(VarType.FP32)
    var transpose_10.tmp_0 : fluid.VarType.LOD_TENSOR.shape(2, 12, 64, 64).astype(VarType.FP32)
    var transpose_10.tmp_1 : fluid.VarType.LOD_TENSOR.shape(0, 2, 64, 12, 64).astype(VarType.FP32)
    var scale_3.tmp_0 : fluid.VarType.LOD_TENSOR.shape(2, 12, 64, 64).astype(VarType.FP32)
    var matmul_5.tmp_0 : fluid.VarType.LOD_TENSOR.shape(2, 12, 64, 64).astype(VarType.FP32)
    var tmp_9 : fluid.VarType.LOD_TENSOR.shape(2, 12, 64, 64).astype(VarType.FP32)
    var softmax_2.tmp_0 : fluid.VarType.LOD_TENSOR.shape(2, 12, 64, 64).astype(VarType.FP32)
    var matmul_6.tmp_0 : fluid.VarType.LOD_TENSOR.shape(2, 12, 64, 64).astype(VarType.FP32)
    var transpose_11.tmp_0 : fluid.VarType.LOD_TENSOR.shape(2, 64, 12, 64).astype(VarType.FP32)
    var transpose_11.tmp_1 : fluid.VarType.LOD_TENSOR.shape(0, 2, 12, 64, 64).astype(VarType.FP32)
    var reshape2_11.tmp_0 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var reshape2_11.tmp_1 : fluid.VarType.LOD_TENSOR.shape(0, 2, 64, 12, 64).astype(VarType.FP32)
    persist trainable param encoder_layer_2_multi_head_att_output_fc.w_0 : fluid.VarType.LOD_TENSOR.shape(768, 768).astype(VarType.FP32)
    var fc_15.tmp_0 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    persist trainable param encoder_layer_2_multi_head_att_output_fc.b_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var fc_15.tmp_1 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var tmp_10 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    persist trainable param encoder_layer_2_post_att_layer_norm_scale : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    persist trainable param encoder_layer_2_post_att_layer_norm_bias : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var layer_norm_5.tmp_0 : fluid.VarType.LOD_TENSOR.shape(128,).astype(VarType.FP32)
    var layer_norm_5.tmp_1 : fluid.VarType.LOD_TENSOR.shape(128,).astype(VarType.FP32)
    var layer_norm_5.tmp_2 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    persist trainable param encoder_layer_2_ffn_fc_0.w_0 : fluid.VarType.LOD_TENSOR.shape(768, 3072).astype(VarType.FP32)
    var fc_16.tmp_0 : fluid.VarType.LOD_TENSOR.shape(2, 64, 3072).astype(VarType.FP32)
    persist trainable param encoder_layer_2_ffn_fc_0.b_0 : fluid.VarType.LOD_TENSOR.shape(3072,).astype(VarType.FP32)
    var fc_16.tmp_1 : fluid.VarType.LOD_TENSOR.shape(2, 64, 3072).astype(VarType.FP32)
    var fc_16.tmp_2 : fluid.VarType.LOD_TENSOR.shape(2, 64, 3072).astype(VarType.FP32)
    persist trainable param encoder_layer_2_ffn_fc_1.w_0 : fluid.VarType.LOD_TENSOR.shape(3072, 768).astype(VarType.FP32)
    var fc_17.tmp_0 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    persist trainable param encoder_layer_2_ffn_fc_1.b_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var fc_17.tmp_1 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var tmp_11 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    persist trainable param encoder_layer_2_post_ffn_layer_norm_scale : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    persist trainable param encoder_layer_2_post_ffn_layer_norm_bias : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var layer_norm_6.tmp_0 : fluid.VarType.LOD_TENSOR.shape(128,).astype(VarType.FP32)
    var layer_norm_6.tmp_1 : fluid.VarType.LOD_TENSOR.shape(128,).astype(VarType.FP32)
    var layer_norm_6.tmp_2 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var slice_0.tmp_0 : fluid.VarType.LOD_TENSOR.shape(2, 1, 768).astype(VarType.FP32)
    persist trainable param pooled_fc.w_0 : fluid.VarType.LOD_TENSOR.shape(768, 768).astype(VarType.FP32)
    var fc_18.tmp_0 : fluid.VarType.LOD_TENSOR.shape(2, 768).astype(VarType.FP32)
    persist trainable param pooled_fc.b_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var fc_18.tmp_1 : fluid.VarType.LOD_TENSOR.shape(2, 768).astype(VarType.FP32)
    var fc_18.tmp_2 : fluid.VarType.LOD_TENSOR.shape(2, 768).astype(VarType.FP32)
    var reshape2_12.tmp_0 : fluid.VarType.LOD_TENSOR.shape(128, 768).astype(VarType.FP32)
    var reshape2_12.tmp_1 : fluid.VarType.LOD_TENSOR.shape(0, 2, 64, 768).astype(VarType.FP32)
    var gather_0.tmp_0 : fluid.VarType.LOD_TENSOR.shape(-1, 768).astype(VarType.FP32)
    persist trainable param mask_lm_trans_fc.w_0 : fluid.VarType.LOD_TENSOR.shape(768, 768).astype(VarType.FP32)
    var fc_19.tmp_0 : fluid.VarType.LOD_TENSOR.shape(-1, 768).astype(VarType.FP32)
    persist trainable param mask_lm_trans_fc.b_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var fc_19.tmp_1 : fluid.VarType.LOD_TENSOR.shape(-1, 768).astype(VarType.FP32)
    var fc_19.tmp_2 : fluid.VarType.LOD_TENSOR.shape(-1, 768).astype(VarType.FP32)
    persist trainable param mask_lm_trans_layer_norm_scale : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    persist trainable param mask_lm_trans_layer_norm_bias : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var layer_norm_7.tmp_0 : fluid.VarType.LOD_TENSOR.shape(-1,).astype(VarType.FP32)
    var layer_norm_7.tmp_1 : fluid.VarType.LOD_TENSOR.shape(-1,).astype(VarType.FP32)
    var layer_norm_7.tmp_2 : fluid.VarType.LOD_TENSOR.shape(-1, 768).astype(VarType.FP32)
    var matmul_7.tmp_0 : fluid.VarType.LOD_TENSOR.shape(-1, 19000).astype(VarType.FP32)
    persist trainable param mask_lm_out_fc.b_0 : fluid.VarType.LOD_TENSOR.shape(19000,).astype(VarType.FP32)
    var tmp_12 : fluid.VarType.LOD_TENSOR.shape(-1, 19000).astype(VarType.FP32)
    var softmax_with_cross_entropy_0.tmp_0 : fluid.VarType.LOD_TENSOR.shape(-1, 19000).astype(VarType.FP32)
    var softmax_with_cross_entropy_0.tmp_1 : fluid.VarType.LOD_TENSOR.shape(-1, 1).astype(VarType.FP32)
    var mean_0.tmp_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var tmp_13 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    persist var learning_rate_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var tmp_14 : fluid.VarType.LOD_TENSOR.shape(19000, 768).astype(VarType.FP32)
    var tmp_15 : fluid.VarType.LOD_TENSOR.shape(2048, 768).astype(VarType.FP32)
    var tmp_16 : fluid.VarType.LOD_TENSOR.shape(4, 768).astype(VarType.FP32)
    var tmp_17 : fluid.VarType.LOD_TENSOR.shape(3, 768).astype(VarType.FP32)
    var tmp_18 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var tmp_19 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var tmp_20 : fluid.VarType.LOD_TENSOR.shape(768, 768).astype(VarType.FP32)
    var tmp_21 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var tmp_22 : fluid.VarType.LOD_TENSOR.shape(768, 768).astype(VarType.FP32)
    var tmp_23 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var tmp_24 : fluid.VarType.LOD_TENSOR.shape(768, 768).astype(VarType.FP32)
    var tmp_25 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var tmp_26 : fluid.VarType.LOD_TENSOR.shape(768, 768).astype(VarType.FP32)
    var tmp_27 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var tmp_28 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var tmp_29 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var tmp_30 : fluid.VarType.LOD_TENSOR.shape(768, 3072).astype(VarType.FP32)
    var tmp_31 : fluid.VarType.LOD_TENSOR.shape(3072,).astype(VarType.FP32)
    var tmp_32 : fluid.VarType.LOD_TENSOR.shape(3072, 768).astype(VarType.FP32)
    var tmp_33 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var tmp_34 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var tmp_35 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var tmp_36 : fluid.VarType.LOD_TENSOR.shape(768, 768).astype(VarType.FP32)
    var tmp_37 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var tmp_38 : fluid.VarType.LOD_TENSOR.shape(768, 768).astype(VarType.FP32)
    var tmp_39 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var tmp_40 : fluid.VarType.LOD_TENSOR.shape(768, 768).astype(VarType.FP32)
    var tmp_41 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var tmp_42 : fluid.VarType.LOD_TENSOR.shape(768, 768).astype(VarType.FP32)
    var tmp_43 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var tmp_44 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var tmp_45 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var tmp_46 : fluid.VarType.LOD_TENSOR.shape(768, 3072).astype(VarType.FP32)
    var tmp_47 : fluid.VarType.LOD_TENSOR.shape(3072,).astype(VarType.FP32)
    var tmp_48 : fluid.VarType.LOD_TENSOR.shape(3072, 768).astype(VarType.FP32)
    var tmp_49 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var tmp_50 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var tmp_51 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var tmp_52 : fluid.VarType.LOD_TENSOR.shape(768, 768).astype(VarType.FP32)
    var tmp_53 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var tmp_54 : fluid.VarType.LOD_TENSOR.shape(768, 768).astype(VarType.FP32)
    var tmp_55 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var tmp_56 : fluid.VarType.LOD_TENSOR.shape(768, 768).astype(VarType.FP32)
    var tmp_57 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var tmp_58 : fluid.VarType.LOD_TENSOR.shape(768, 768).astype(VarType.FP32)
    var tmp_59 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var tmp_60 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var tmp_61 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var tmp_62 : fluid.VarType.LOD_TENSOR.shape(768, 3072).astype(VarType.FP32)
    var tmp_63 : fluid.VarType.LOD_TENSOR.shape(3072,).astype(VarType.FP32)
    var tmp_64 : fluid.VarType.LOD_TENSOR.shape(3072, 768).astype(VarType.FP32)
    var tmp_65 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var tmp_66 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var tmp_67 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var tmp_68 : fluid.VarType.LOD_TENSOR.shape(768, 768).astype(VarType.FP32)
    var tmp_69 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var tmp_70 : fluid.VarType.LOD_TENSOR.shape(768, 768).astype(VarType.FP32)
    var tmp_71 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var tmp_72 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var tmp_73 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var tmp_74 : fluid.VarType.LOD_TENSOR.shape(19000,).astype(VarType.FP32)
    var mean_1.tmp_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var pos_embedding@GRAD : fluid.VarType.LOD_TENSOR.shape(2048, 768).astype(VarType.FP32)
    var task_embedding@GRAD : fluid.VarType.LOD_TENSOR.shape(3, 768).astype(VarType.FP32)
    var embedding_0.tmp_0@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var tmp_0@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var tmp_2@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var pre_encoder_layer_norm_scale@GRAD : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var layer_norm_0.tmp_2@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var encoder_layer_0_multi_head_att_query_fc.w_0@GRAD : fluid.VarType.LOD_TENSOR.shape(768, 768).astype(VarType.FP32)
    var encoder_layer_0_multi_head_att_query_fc.b_0@GRAD : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var fc_0.tmp_0@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var encoder_layer_0_multi_head_att_key_fc.w_0@GRAD : fluid.VarType.LOD_TENSOR.shape(768, 768).astype(VarType.FP32)
    var encoder_layer_0_multi_head_att_key_fc.b_0@GRAD : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var encoder_layer_0_multi_head_att_value_fc.w_0@GRAD : fluid.VarType.LOD_TENSOR.shape(768, 768).astype(VarType.FP32)
    var fc_2.tmp_0@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var fc_0.tmp_1@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var reshape2_1.tmp_0@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 12, 64).astype(VarType.FP32)
    var reshape2_2.tmp_0@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 12, 64).astype(VarType.FP32)
    var transpose_0.tmp_0@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 12, 64, 64).astype(VarType.FP32)
    var layer_norm_0.tmp_2@GRAD@RENAME@block0@3 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var transpose_1.tmp_0@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 12, 64, 64).astype(VarType.FP32)
    var embedding_3.tmp_0@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var matmul_1.tmp_0@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 12, 64, 64).astype(VarType.FP32)
    var transpose_2.tmp_0@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 12, 64, 64).astype(VarType.FP32)
    var matmul_2.tmp_0@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 12, 64, 64).astype(VarType.FP32)
    var encoder_layer_0_multi_head_att_output_fc.w_0@GRAD : fluid.VarType.LOD_TENSOR.shape(768, 768).astype(VarType.FP32)
    var encoder_layer_0_multi_head_att_output_fc.b_0@GRAD : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var transpose_3.tmp_0@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 12, 64).astype(VarType.FP32)
    var layer_norm_0.tmp_2@GRAD@RENAME@block0@0 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var tmp_4@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var encoder_layer_0_post_att_layer_norm_scale@GRAD : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var encoder_layer_0_post_att_layer_norm_bias@GRAD : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var encoder_layer_0_ffn_fc_0.w_0@GRAD : fluid.VarType.LOD_TENSOR.shape(768, 3072).astype(VarType.FP32)
    var layer_norm_1.tmp_2@GRAD@RENAME@block0@1 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var fc_4.tmp_0@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 3072).astype(VarType.FP32)
    var fc_4.tmp_2@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 3072).astype(VarType.FP32)
    var encoder_layer_0_ffn_fc_1.b_0@GRAD : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var fc_5.tmp_0@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var layer_norm_1.tmp_2@GRAD@RENAME@block0@0 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var tmp_5@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var reshape2_3.tmp_0@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var encoder_layer_0_post_ffn_layer_norm_scale@GRAD : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var word_embedding@GRAD : fluid.VarType.LOD_TENSOR.shape(19000, 768).astype(VarType.FP32)
    var encoder_layer_0_post_ffn_layer_norm_bias@GRAD : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var layer_norm_2.tmp_2@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var encoder_layer_1_multi_head_att_query_fc.w_0@GRAD : fluid.VarType.LOD_TENSOR.shape(768, 768).astype(VarType.FP32)
    var encoder_layer_1_multi_head_att_key_fc.w_0@GRAD : fluid.VarType.LOD_TENSOR.shape(768, 768).astype(VarType.FP32)
    var layer_norm_2.tmp_2@GRAD@RENAME@block0@2 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var encoder_layer_1_multi_head_att_key_fc.b_0@GRAD : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var layer_norm_2.tmp_2@GRAD@RENAME@block0@1 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var encoder_layer_1_multi_head_att_value_fc.b_0@GRAD : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var fc_7.tmp_1@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var fc_8.tmp_1@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var reshape2_6.tmp_0@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 12, 64).astype(VarType.FP32)
    var transpose_5.tmp_0@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 12, 64, 64).astype(VarType.FP32)
    var scale_2.tmp_0@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 12, 64, 64).astype(VarType.FP32)
    var transpose_6.tmp_0@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 12, 64, 64).astype(VarType.FP32)
    var reshape2_0.tmp_0@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 12, 64).astype(VarType.FP32)
    var softmax_1.tmp_0@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 12, 64, 64).astype(VarType.FP32)
    var matmul_4.tmp_0@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 12, 64, 64).astype(VarType.FP32)
    var transpose_7.tmp_0@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 12, 64).astype(VarType.FP32)
    var encoder_layer_1_multi_head_att_output_fc.w_0@GRAD : fluid.VarType.LOD_TENSOR.shape(768, 768).astype(VarType.FP32)
    var reshape2_7.tmp_0@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var encoder_layer_1_multi_head_att_output_fc.b_0@GRAD : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var fc_9.tmp_0@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var layer_norm_2.tmp_2@GRAD@RENAME@block0@0 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var fc_9.tmp_1@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var encoder_layer_1_post_att_layer_norm_scale@GRAD : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var encoder_layer_1_post_att_layer_norm_bias@GRAD : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var layer_norm_3.tmp_2@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var encoder_layer_1_ffn_fc_0.w_0@GRAD : fluid.VarType.LOD_TENSOR.shape(768, 3072).astype(VarType.FP32)
    var fc_10.tmp_1@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 3072).astype(VarType.FP32)
    var encoder_layer_1_ffn_fc_1.w_0@GRAD : fluid.VarType.LOD_TENSOR.shape(3072, 768).astype(VarType.FP32)
    var encoder_layer_1_ffn_fc_1.b_0@GRAD : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var fc_11.tmp_0@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var layer_norm_3.tmp_2@GRAD@RENAME@block0@0 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var fc_11.tmp_1@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var encoder_layer_1_post_ffn_layer_norm_scale@GRAD : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var layer_norm_4.tmp_2@GRAD@RENAME@block0@3 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var encoder_layer_2_multi_head_att_key_fc.w_0@GRAD : fluid.VarType.LOD_TENSOR.shape(768, 768).astype(VarType.FP32)
    var fc_5.tmp_1@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var fc_13.tmp_0@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var encoder_layer_2_multi_head_att_value_fc.w_0@GRAD : fluid.VarType.LOD_TENSOR.shape(768, 768).astype(VarType.FP32)
    var word_embedding@GRAD@RENAME@block0@1 : fluid.VarType.LOD_TENSOR.shape(19000, 768).astype(VarType.FP32)
    var layer_norm_4.tmp_2@GRAD@RENAME@block0@1 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var fc_14.tmp_0@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var fc_12.tmp_1@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var fc_13.tmp_1@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var transpose_9.tmp_0@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 12, 64, 64).astype(VarType.FP32)
    var scale_3.tmp_0@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 12, 64, 64).astype(VarType.FP32)
    var matmul_5.tmp_0@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 12, 64, 64).astype(VarType.FP32)
    var tmp_9@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 12, 64, 64).astype(VarType.FP32)
    var transpose_10.tmp_0@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 12, 64, 64).astype(VarType.FP32)
    var softmax_2.tmp_0@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 12, 64, 64).astype(VarType.FP32)
    var encoder_layer_2_multi_head_att_output_fc.w_0@GRAD : fluid.VarType.LOD_TENSOR.shape(768, 768).astype(VarType.FP32)
    var reshape2_11.tmp_0@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var encoder_layer_1_post_ffn_layer_norm_bias@GRAD : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var encoder_layer_2_multi_head_att_output_fc.b_0@GRAD : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var fc_15.tmp_0@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var layer_norm_4.tmp_2@GRAD@RENAME@block0@0 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var transpose_11.tmp_0@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 12, 64).astype(VarType.FP32)
    var sent_embedding@GRAD : fluid.VarType.LOD_TENSOR.shape(4, 768).astype(VarType.FP32)
    var reshape2_10.tmp_0@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 12, 64).astype(VarType.FP32)
    var encoder_layer_0_ffn_fc_1.w_0@GRAD : fluid.VarType.LOD_TENSOR.shape(3072, 768).astype(VarType.FP32)
    var tmp_6@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 12, 64, 64).astype(VarType.FP32)
    var fc_6.tmp_0@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var encoder_layer_1_multi_head_att_value_fc.w_0@GRAD : fluid.VarType.LOD_TENSOR.shape(768, 768).astype(VarType.FP32)
    var fc_16.tmp_2@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 3072).astype(VarType.FP32)
    var mask_lm_trans_fc.b_0@GRAD : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var fc_17.tmp_0@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var encoder_layer_2_post_ffn_layer_norm_bias@GRAD : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var encoder_layer_2_multi_head_att_key_fc.b_0@GRAD : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var fc_15.tmp_1@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var reshape2_9.tmp_0@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 12, 64).astype(VarType.FP32)
    var encoder_layer_2_post_att_layer_norm_bias@GRAD : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var fc_10.tmp_2@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 3072).astype(VarType.FP32)
    var transpose_8.tmp_0@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 12, 64, 64).astype(VarType.FP32)
    var reshape2_4.tmp_0@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 12, 64).astype(VarType.FP32)
    var reshape2_8.tmp_0@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 12, 64).astype(VarType.FP32)
    var embedding_2.tmp_0@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var layer_norm_0.tmp_2@GRAD@RENAME@block0@2 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var encoder_layer_2_ffn_fc_1.b_0@GRAD : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var fc_4.tmp_1@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 3072).astype(VarType.FP32)
    var fc_2.tmp_1@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var fc_3.tmp_0@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var layer_norm_2.tmp_2@GRAD@RENAME@block0@3 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var pre_encoder_layer_norm_bias@GRAD : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var reshape2_5.tmp_0@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 12, 64).astype(VarType.FP32)
    var layer_norm_0.tmp_2@GRAD@RENAME@block0@1 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var layer_norm_4.tmp_2@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var encoder_layer_2_multi_head_att_query_fc.w_0@GRAD : fluid.VarType.LOD_TENSOR.shape(768, 768).astype(VarType.FP32)
    var tmp_7@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var fc_19.tmp_2@GRAD : fluid.VarType.LOD_TENSOR.shape(-1, 768).astype(VarType.FP32)
    var word_embedding@GRAD@RENAME@block0@0 : fluid.VarType.LOD_TENSOR.shape(19000, 768).astype(VarType.FP32)
    var tmp_8@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var encoder_layer_1_multi_head_att_query_fc.b_0@GRAD : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var matmul_3.tmp_0@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 12, 64, 64).astype(VarType.FP32)
    var encoder_layer_0_ffn_fc_0.b_0@GRAD : fluid.VarType.LOD_TENSOR.shape(3072,).astype(VarType.FP32)
    var matmul_6.tmp_0@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 12, 64, 64).astype(VarType.FP32)
    var layer_norm_1.tmp_2@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var softmax_0.tmp_0@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 12, 64, 64).astype(VarType.FP32)
    var tmp_13@GRAD : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var tmp_1@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var transpose_4.tmp_0@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 12, 64, 64).astype(VarType.FP32)
    var fc_12.tmp_0@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var fc_1.tmp_0@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var fc_10.tmp_0@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 3072).astype(VarType.FP32)
    var fc_3.tmp_1@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var fc_14.tmp_1@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var layer_norm_5.tmp_2@GRAD@RENAME@block0@1 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var fc_19.tmp_0@GRAD : fluid.VarType.LOD_TENSOR.shape(-1, 768).astype(VarType.FP32)
    var gather_0.tmp_0@GRAD : fluid.VarType.LOD_TENSOR.shape(-1, 768).astype(VarType.FP32)
    var tmp_3@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 12, 64, 64).astype(VarType.FP32)
    var fc_7.tmp_0@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var mask_lm_trans_layer_norm_scale@GRAD : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var encoder_layer_0_multi_head_att_value_fc.b_0@GRAD : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var fc_6.tmp_1@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var encoder_layer_2_multi_head_att_value_fc.b_0@GRAD : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var layer_norm_6.tmp_2@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var mean_1.tmp_0@GRAD : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var fc_8.tmp_0@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var encoder_layer_2_ffn_fc_0.w_0@GRAD : fluid.VarType.LOD_TENSOR.shape(768, 3072).astype(VarType.FP32)
    var layer_norm_4.tmp_2@GRAD@RENAME@block0@2 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var fc_1.tmp_1@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var fc_19.tmp_1@GRAD : fluid.VarType.LOD_TENSOR.shape(-1, 768).astype(VarType.FP32)
    var encoder_layer_1_ffn_fc_0.b_0@GRAD : fluid.VarType.LOD_TENSOR.shape(3072,).astype(VarType.FP32)
    var mean_0.tmp_0@GRAD : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var softmax_with_cross_entropy_0.tmp_1@GRAD : fluid.VarType.LOD_TENSOR.shape(-1, 1).astype(VarType.FP32)
    var tmp_12@GRAD : fluid.VarType.LOD_TENSOR.shape(-1, 19000).astype(VarType.FP32)
    var matmul_7.tmp_0@GRAD : fluid.VarType.LOD_TENSOR.shape(-1, 19000).astype(VarType.FP32)
    var mask_lm_out_fc.b_0@GRAD : fluid.VarType.LOD_TENSOR.shape(19000,).astype(VarType.FP32)
    var layer_norm_7.tmp_2@GRAD : fluid.VarType.LOD_TENSOR.shape(-1, 768).astype(VarType.FP32)
    var mask_lm_trans_layer_norm_bias@GRAD : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var embedding_1.tmp_0@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var layer_norm_5.tmp_2@GRAD@RENAME@block0@0 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var mask_lm_trans_fc.w_0@GRAD : fluid.VarType.LOD_TENSOR.shape(768, 768).astype(VarType.FP32)
    var encoder_layer_2_post_ffn_layer_norm_scale@GRAD : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var reshape2_12.tmp_0@GRAD : fluid.VarType.LOD_TENSOR.shape(128, 768).astype(VarType.FP32)
    var layer_norm_3.tmp_2@GRAD@RENAME@block0@1 : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var tmp_11@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var fc_17.tmp_1@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var layer_norm_5.tmp_2@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var encoder_layer_2_ffn_fc_1.w_0@GRAD : fluid.VarType.LOD_TENSOR.shape(3072, 768).astype(VarType.FP32)
    var encoder_layer_2_multi_head_att_query_fc.b_0@GRAD : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var fc_16.tmp_1@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 3072).astype(VarType.FP32)
    var fc_16.tmp_0@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 3072).astype(VarType.FP32)
    var scale_1.tmp_0@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 12, 64, 64).astype(VarType.FP32)
    var encoder_layer_2_ffn_fc_0.b_0@GRAD : fluid.VarType.LOD_TENSOR.shape(3072,).astype(VarType.FP32)
    var encoder_layer_2_post_att_layer_norm_scale@GRAD : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var tmp_10@GRAD : fluid.VarType.LOD_TENSOR.shape(2, 64, 768).astype(VarType.FP32)
    var fill_constant_1.tmp_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var square_0.tmp_0 : fluid.VarType.LOD_TENSOR.shape(3072,).astype(VarType.FP32)
    var reduce_sum_0.tmp_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var square_1.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768, 3072).astype(VarType.FP32)
    var reduce_sum_1.tmp_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var square_2.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var reduce_sum_2.tmp_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var square_3.tmp_0 : fluid.VarType.LOD_TENSOR.shape(3072, 768).astype(VarType.FP32)
    var reduce_sum_3.tmp_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var square_4.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var reduce_sum_4.tmp_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var square_5.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768, 768).astype(VarType.FP32)
    var reduce_sum_5.tmp_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var square_6.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var reduce_sum_6.tmp_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var square_7.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768, 768).astype(VarType.FP32)
    var reduce_sum_7.tmp_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var square_8.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var reduce_sum_8.tmp_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var square_9.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768, 768).astype(VarType.FP32)
    var reduce_sum_9.tmp_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var square_10.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var reduce_sum_10.tmp_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var square_11.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768, 768).astype(VarType.FP32)
    var reduce_sum_11.tmp_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var square_12.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var reduce_sum_12.tmp_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var square_13.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var reduce_sum_13.tmp_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var square_14.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var reduce_sum_14.tmp_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var square_15.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var reduce_sum_15.tmp_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var square_16.tmp_0 : fluid.VarType.LOD_TENSOR.shape(3072,).astype(VarType.FP32)
    var reduce_sum_16.tmp_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var square_17.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768, 3072).astype(VarType.FP32)
    var reduce_sum_17.tmp_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var square_18.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var reduce_sum_18.tmp_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var square_19.tmp_0 : fluid.VarType.LOD_TENSOR.shape(3072, 768).astype(VarType.FP32)
    var reduce_sum_19.tmp_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var square_20.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var reduce_sum_20.tmp_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var square_21.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768, 768).astype(VarType.FP32)
    var reduce_sum_21.tmp_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var square_22.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var reduce_sum_22.tmp_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var square_23.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768, 768).astype(VarType.FP32)
    var reduce_sum_23.tmp_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var square_24.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var reduce_sum_24.tmp_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var square_25.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768, 768).astype(VarType.FP32)
    var reduce_sum_25.tmp_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var square_26.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var reduce_sum_26.tmp_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var square_27.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768, 768).astype(VarType.FP32)
    var reduce_sum_27.tmp_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var square_28.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var reduce_sum_28.tmp_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var square_29.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var reduce_sum_29.tmp_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var square_30.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var reduce_sum_30.tmp_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var square_31.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var reduce_sum_31.tmp_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var square_32.tmp_0 : fluid.VarType.LOD_TENSOR.shape(3072,).astype(VarType.FP32)
    var reduce_sum_32.tmp_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var square_33.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768, 3072).astype(VarType.FP32)
    var reduce_sum_33.tmp_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var square_34.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var reduce_sum_34.tmp_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var square_35.tmp_0 : fluid.VarType.LOD_TENSOR.shape(3072, 768).astype(VarType.FP32)
    var reduce_sum_35.tmp_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var square_36.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var reduce_sum_36.tmp_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var square_37.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768, 768).astype(VarType.FP32)
    var reduce_sum_37.tmp_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var square_38.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var reduce_sum_38.tmp_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var square_39.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768, 768).astype(VarType.FP32)
    var reduce_sum_39.tmp_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var square_40.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var reduce_sum_40.tmp_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var square_41.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768, 768).astype(VarType.FP32)
    var reduce_sum_41.tmp_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var square_42.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var reduce_sum_42.tmp_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var square_43.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768, 768).astype(VarType.FP32)
    var reduce_sum_43.tmp_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var square_44.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var reduce_sum_44.tmp_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var square_45.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var reduce_sum_45.tmp_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var square_46.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var reduce_sum_46.tmp_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var square_47.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var reduce_sum_47.tmp_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var square_48.tmp_0 : fluid.VarType.LOD_TENSOR.shape(19000,).astype(VarType.FP32)
    var reduce_sum_48.tmp_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var square_49.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var reduce_sum_49.tmp_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var square_50.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768, 768).astype(VarType.FP32)
    var reduce_sum_50.tmp_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var square_51.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var reduce_sum_51.tmp_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var square_52.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var reduce_sum_52.tmp_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var square_53.tmp_0 : fluid.VarType.LOD_TENSOR.shape(2048, 768).astype(VarType.FP32)
    var reduce_sum_53.tmp_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var square_54.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var reduce_sum_54.tmp_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var square_55.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var reduce_sum_55.tmp_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var square_56.tmp_0 : fluid.VarType.LOD_TENSOR.shape(4, 768).astype(VarType.FP32)
    var reduce_sum_56.tmp_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var square_57.tmp_0 : fluid.VarType.LOD_TENSOR.shape(3, 768).astype(VarType.FP32)
    var reduce_sum_57.tmp_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var square_58.tmp_0 : fluid.VarType.LOD_TENSOR.shape(19000, 768).astype(VarType.FP32)
    var reduce_sum_58.tmp_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var sum_0.tmp_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var sqrt_0.tmp_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var elementwise_max_0.tmp_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var elementwise_div_0.tmp_0 : fluid.VarType.LOD_TENSOR.shape(1,).astype(VarType.FP32)
    var elementwise_mul_0.tmp_0 : fluid.VarType.LOD_TENSOR.shape(3072,).astype(VarType.FP32)
    var elementwise_mul_1.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768, 3072).astype(VarType.FP32)
    var elementwise_mul_2.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var elementwise_mul_3.tmp_0 : fluid.VarType.LOD_TENSOR.shape(3072, 768).astype(VarType.FP32)
    var elementwise_mul_4.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var elementwise_mul_5.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768, 768).astype(VarType.FP32)
    var elementwise_mul_6.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var elementwise_mul_7.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768, 768).astype(VarType.FP32)
    var elementwise_mul_8.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var elementwise_mul_9.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768, 768).astype(VarType.FP32)
    var elementwise_mul_10.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var elementwise_mul_11.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768, 768).astype(VarType.FP32)
    var elementwise_mul_12.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var elementwise_mul_13.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var elementwise_mul_14.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var elementwise_mul_15.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var elementwise_mul_16.tmp_0 : fluid.VarType.LOD_TENSOR.shape(3072,).astype(VarType.FP32)
    var elementwise_mul_17.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768, 3072).astype(VarType.FP32)
    var elementwise_mul_18.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var elementwise_mul_19.tmp_0 : fluid.VarType.LOD_TENSOR.shape(3072, 768).astype(VarType.FP32)
    var elementwise_mul_20.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var elementwise_mul_21.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768, 768).astype(VarType.FP32)
    var elementwise_mul_22.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var elementwise_mul_23.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768, 768).astype(VarType.FP32)
    var elementwise_mul_24.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var elementwise_mul_25.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768, 768).astype(VarType.FP32)
    var elementwise_mul_26.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var elementwise_mul_27.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768, 768).astype(VarType.FP32)
    var elementwise_mul_28.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var elementwise_mul_29.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var elementwise_mul_30.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var elementwise_mul_31.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var elementwise_mul_32.tmp_0 : fluid.VarType.LOD_TENSOR.shape(3072,).astype(VarType.FP32)
    var elementwise_mul_33.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768, 3072).astype(VarType.FP32)
    var elementwise_mul_34.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var elementwise_mul_35.tmp_0 : fluid.VarType.LOD_TENSOR.shape(3072, 768).astype(VarType.FP32)
    var elementwise_mul_36.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var elementwise_mul_37.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768, 768).astype(VarType.FP32)
    var elementwise_mul_38.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var elementwise_mul_39.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768, 768).astype(VarType.FP32)
    var elementwise_mul_40.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var elementwise_mul_41.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768, 768).astype(VarType.FP32)
    var elementwise_mul_42.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var elementwise_mul_43.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768, 768).astype(VarType.FP32)
    var elementwise_mul_44.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var elementwise_mul_45.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var elementwise_mul_46.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var elementwise_mul_47.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var elementwise_mul_48.tmp_0 : fluid.VarType.LOD_TENSOR.shape(19000,).astype(VarType.FP32)
    var elementwise_mul_49.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var elementwise_mul_50.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768, 768).astype(VarType.FP32)
    var elementwise_mul_51.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var elementwise_mul_52.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var elementwise_mul_53.tmp_0 : fluid.VarType.LOD_TENSOR.shape(2048, 768).astype(VarType.FP32)
    var elementwise_mul_54.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var elementwise_mul_55.tmp_0 : fluid.VarType.LOD_TENSOR.shape(768,).astype(VarType.FP32)
    var elementwise_mul_56.tmp_0 : fluid.VarType.LOD_TENSOR.shape(4, 768).astype(VarType.FP32)
    var elementwise_mul_57.tmp_0 : fluid.VarType.LOD_TENSOR.shape(3, 768).astype(VarType.FP32)
    var elementwise_mul_58.tmp_0 : fluid.VarType.LOD_TENSOR.shape(19000, 768).astype(VarType.FP32)

    {Out=['embedding_0.tmp_0']} = lookup_table(inputs={Ids=['src_ids'], W=['word_embedding']}, entry = none, entry_config = , epmap = [], grad_inplace = False, height_sections = [], is_distributed = False, is_sparse = False, is_test = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_idx = -1, remote_prefetch = False, table_names = [], trainer_id = 0)
    {Out=['embedding_1.tmp_0']} = lookup_table(inputs={Ids=['pos_ids'], W=['pos_embedding']}, entry = none, entry_config = , epmap = [], grad_inplace = False, height_sections = [], is_distributed = False, is_sparse = False, is_test = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_idx = -1, remote_prefetch = False, table_names = [], trainer_id = 0)
    {Out=['embedding_2.tmp_0']} = lookup_table(inputs={Ids=['sent_ids'], W=['sent_embedding']}, entry = none, entry_config = , epmap = [], grad_inplace = False, height_sections = [], is_distributed = False, is_sparse = False, is_test = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_idx = -1, remote_prefetch = False, table_names = [], trainer_id = 0)
    {Out=['embedding_3.tmp_0']} = lookup_table(inputs={Ids=['task_ids'], W=['task_embedding']}, entry = none, entry_config = , epmap = [], grad_inplace = False, height_sections = [], is_distributed = False, is_sparse = False, is_test = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_idx = -1, remote_prefetch = False, table_names = [], trainer_id = 0)
    {Out=['tmp_0']} = elementwise_add(inputs={X=['embedding_0.tmp_0'], Y=['embedding_1.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_1']} = elementwise_add(inputs={X=['tmp_0'], Y=['embedding_2.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_2']} = elementwise_add(inputs={X=['tmp_1'], Y=['embedding_3.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_0.tmp_0'], Variance=['layer_norm_0.tmp_1'], Y=['layer_norm_0.tmp_2']} = layer_norm(inputs={Bias=['pre_encoder_layer_norm_bias'], Scale=['pre_encoder_layer_norm_scale'], X=['tmp_2']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = /, op_role = 0, op_role_var = [])
    {Out=['matmul_0.tmp_0']} = matmul(inputs={X=['input_mask'], Y=['input_mask']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = True, use_mkldnn = False, use_quantizer = False)
    {Out=['scale_0.tmp_0']} = scale(inputs={ScaleTensor=[], X=['matmul_0.tmp_0']}, bias = -1.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 10000.0)
    {Y=['stack_0.tmp_0']} = stack(inputs={X=['scale_0.tmp_0', 'scale_0.tmp_0', 'scale_0.tmp_0', 'scale_0.tmp_0', 'scale_0.tmp_0', 'scale_0.tmp_0', 'scale_0.tmp_0', 'scale_0.tmp_0', 'scale_0.tmp_0', 'scale_0.tmp_0', 'scale_0.tmp_0', 'scale_0.tmp_0']}, axis = 1, op_device = , op_namescope = /, op_role = 0, op_role_var = [])
    {Out=['fc_0.tmp_0']} = mul(inputs={X=['layer_norm_0.tmp_2'], Y=['encoder_layer_0_multi_head_att_query_fc.w_0']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['fc_0.tmp_1']} = elementwise_add(inputs={X=['fc_0.tmp_0'], Y=['encoder_layer_0_multi_head_att_query_fc.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['fc_1.tmp_0']} = mul(inputs={X=['layer_norm_0.tmp_2'], Y=['encoder_layer_0_multi_head_att_key_fc.w_0']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['fc_1.tmp_1']} = elementwise_add(inputs={X=['fc_1.tmp_0'], Y=['encoder_layer_0_multi_head_att_key_fc.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['fc_2.tmp_0']} = mul(inputs={X=['layer_norm_0.tmp_2'], Y=['encoder_layer_0_multi_head_att_value_fc.w_0']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['fc_2.tmp_1']} = elementwise_add(inputs={X=['fc_2.tmp_0'], Y=['encoder_layer_0_multi_head_att_value_fc.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['reshape2_0.tmp_0'], XShape=['reshape2_0.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_0.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_0.tmp_0'], XShape=['transpose_0.tmp_1']} = transpose2(inputs={X=['reshape2_0.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_1.tmp_0'], XShape=['reshape2_1.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_1.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_1.tmp_0'], XShape=['transpose_1.tmp_1']} = transpose2(inputs={X=['reshape2_1.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_2.tmp_0'], XShape=['reshape2_2.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_2.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_2.tmp_0'], XShape=['transpose_2.tmp_1']} = transpose2(inputs={X=['reshape2_2.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['scale_1.tmp_0']} = scale(inputs={ScaleTensor=[], X=['transpose_0.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 0.125)
    {Out=['matmul_1.tmp_0']} = matmul(inputs={X=['scale_1.tmp_0'], Y=['transpose_1.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = True, use_mkldnn = False, use_quantizer = False)
    {Out=['tmp_3']} = elementwise_add(inputs={X=['matmul_1.tmp_0'], Y=['stack_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['softmax_0.tmp_0']} = softmax(inputs={X=['tmp_3']}, axis = -1, data_format = AnyLayout, is_test = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {Out=['matmul_2.tmp_0']} = matmul(inputs={X=['softmax_0.tmp_0'], Y=['transpose_2.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['transpose_3.tmp_0'], XShape=['transpose_3.tmp_1']} = transpose2(inputs={X=['matmul_2.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_3.tmp_0'], XShape=['reshape2_3.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_3.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 768], use_quantizer = False)
    {Out=['fc_3.tmp_0']} = mul(inputs={X=['reshape2_3.tmp_0'], Y=['encoder_layer_0_multi_head_att_output_fc.w_0']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['fc_3.tmp_1']} = elementwise_add(inputs={X=['fc_3.tmp_0'], Y=['encoder_layer_0_multi_head_att_output_fc.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_4']} = elementwise_add(inputs={X=['fc_3.tmp_1'], Y=['layer_norm_0.tmp_2']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_1.tmp_0'], Variance=['layer_norm_1.tmp_1'], Y=['layer_norm_1.tmp_2']} = layer_norm(inputs={Bias=['encoder_layer_0_post_att_layer_norm_bias'], Scale=['encoder_layer_0_post_att_layer_norm_scale'], X=['tmp_4']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = /, op_role = 0, op_role_var = [])
    {Out=['fc_4.tmp_0']} = mul(inputs={X=['layer_norm_1.tmp_2'], Y=['encoder_layer_0_ffn_fc_0.w_0']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['fc_4.tmp_1']} = elementwise_add(inputs={X=['fc_4.tmp_0'], Y=['encoder_layer_0_ffn_fc_0.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['fc_4.tmp_2']} = gelu(inputs={X=['fc_4.tmp_1']}, approximate = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {Out=['fc_5.tmp_0']} = mul(inputs={X=['fc_4.tmp_2'], Y=['encoder_layer_0_ffn_fc_1.w_0']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['fc_5.tmp_1']} = elementwise_add(inputs={X=['fc_5.tmp_0'], Y=['encoder_layer_0_ffn_fc_1.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_5']} = elementwise_add(inputs={X=['fc_5.tmp_1'], Y=['layer_norm_1.tmp_2']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_2.tmp_0'], Variance=['layer_norm_2.tmp_1'], Y=['layer_norm_2.tmp_2']} = layer_norm(inputs={Bias=['encoder_layer_0_post_ffn_layer_norm_bias'], Scale=['encoder_layer_0_post_ffn_layer_norm_scale'], X=['tmp_5']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = /, op_role = 0, op_role_var = [])
    {Out=['fc_6.tmp_0']} = mul(inputs={X=['layer_norm_2.tmp_2'], Y=['encoder_layer_1_multi_head_att_query_fc.w_0']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['fc_6.tmp_1']} = elementwise_add(inputs={X=['fc_6.tmp_0'], Y=['encoder_layer_1_multi_head_att_query_fc.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['fc_7.tmp_0']} = mul(inputs={X=['layer_norm_2.tmp_2'], Y=['encoder_layer_1_multi_head_att_key_fc.w_0']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['fc_7.tmp_1']} = elementwise_add(inputs={X=['fc_7.tmp_0'], Y=['encoder_layer_1_multi_head_att_key_fc.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['fc_8.tmp_0']} = mul(inputs={X=['layer_norm_2.tmp_2'], Y=['encoder_layer_1_multi_head_att_value_fc.w_0']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['fc_8.tmp_1']} = elementwise_add(inputs={X=['fc_8.tmp_0'], Y=['encoder_layer_1_multi_head_att_value_fc.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['reshape2_4.tmp_0'], XShape=['reshape2_4.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_6.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_4.tmp_0'], XShape=['transpose_4.tmp_1']} = transpose2(inputs={X=['reshape2_4.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_5.tmp_0'], XShape=['reshape2_5.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_7.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_5.tmp_0'], XShape=['transpose_5.tmp_1']} = transpose2(inputs={X=['reshape2_5.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_6.tmp_0'], XShape=['reshape2_6.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_8.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_6.tmp_0'], XShape=['transpose_6.tmp_1']} = transpose2(inputs={X=['reshape2_6.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['scale_2.tmp_0']} = scale(inputs={ScaleTensor=[], X=['transpose_4.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 0.125)
    {Out=['matmul_3.tmp_0']} = matmul(inputs={X=['scale_2.tmp_0'], Y=['transpose_5.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = True, use_mkldnn = False, use_quantizer = False)
    {Out=['tmp_6']} = elementwise_add(inputs={X=['matmul_3.tmp_0'], Y=['stack_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['softmax_1.tmp_0']} = softmax(inputs={X=['tmp_6']}, axis = -1, data_format = AnyLayout, is_test = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {Out=['matmul_4.tmp_0']} = matmul(inputs={X=['softmax_1.tmp_0'], Y=['transpose_6.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['transpose_7.tmp_0'], XShape=['transpose_7.tmp_1']} = transpose2(inputs={X=['matmul_4.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_7.tmp_0'], XShape=['reshape2_7.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_7.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 768], use_quantizer = False)
    {Out=['fc_9.tmp_0']} = mul(inputs={X=['reshape2_7.tmp_0'], Y=['encoder_layer_1_multi_head_att_output_fc.w_0']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['fc_9.tmp_1']} = elementwise_add(inputs={X=['fc_9.tmp_0'], Y=['encoder_layer_1_multi_head_att_output_fc.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_7']} = elementwise_add(inputs={X=['fc_9.tmp_1'], Y=['layer_norm_2.tmp_2']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_3.tmp_0'], Variance=['layer_norm_3.tmp_1'], Y=['layer_norm_3.tmp_2']} = layer_norm(inputs={Bias=['encoder_layer_1_post_att_layer_norm_bias'], Scale=['encoder_layer_1_post_att_layer_norm_scale'], X=['tmp_7']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = /, op_role = 0, op_role_var = [])
    {Out=['fc_10.tmp_0']} = mul(inputs={X=['layer_norm_3.tmp_2'], Y=['encoder_layer_1_ffn_fc_0.w_0']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['fc_10.tmp_1']} = elementwise_add(inputs={X=['fc_10.tmp_0'], Y=['encoder_layer_1_ffn_fc_0.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['fc_10.tmp_2']} = gelu(inputs={X=['fc_10.tmp_1']}, approximate = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {Out=['fc_11.tmp_0']} = mul(inputs={X=['fc_10.tmp_2'], Y=['encoder_layer_1_ffn_fc_1.w_0']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['fc_11.tmp_1']} = elementwise_add(inputs={X=['fc_11.tmp_0'], Y=['encoder_layer_1_ffn_fc_1.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_8']} = elementwise_add(inputs={X=['fc_11.tmp_1'], Y=['layer_norm_3.tmp_2']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_4.tmp_0'], Variance=['layer_norm_4.tmp_1'], Y=['layer_norm_4.tmp_2']} = layer_norm(inputs={Bias=['encoder_layer_1_post_ffn_layer_norm_bias'], Scale=['encoder_layer_1_post_ffn_layer_norm_scale'], X=['tmp_8']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = /, op_role = 0, op_role_var = [])
    {Out=['fc_12.tmp_0']} = mul(inputs={X=['layer_norm_4.tmp_2'], Y=['encoder_layer_2_multi_head_att_query_fc.w_0']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['fc_12.tmp_1']} = elementwise_add(inputs={X=['fc_12.tmp_0'], Y=['encoder_layer_2_multi_head_att_query_fc.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['fc_13.tmp_0']} = mul(inputs={X=['layer_norm_4.tmp_2'], Y=['encoder_layer_2_multi_head_att_key_fc.w_0']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['fc_13.tmp_1']} = elementwise_add(inputs={X=['fc_13.tmp_0'], Y=['encoder_layer_2_multi_head_att_key_fc.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['fc_14.tmp_0']} = mul(inputs={X=['layer_norm_4.tmp_2'], Y=['encoder_layer_2_multi_head_att_value_fc.w_0']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['fc_14.tmp_1']} = elementwise_add(inputs={X=['fc_14.tmp_0'], Y=['encoder_layer_2_multi_head_att_value_fc.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['reshape2_8.tmp_0'], XShape=['reshape2_8.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_12.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_8.tmp_0'], XShape=['transpose_8.tmp_1']} = transpose2(inputs={X=['reshape2_8.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_9.tmp_0'], XShape=['reshape2_9.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_13.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_9.tmp_0'], XShape=['transpose_9.tmp_1']} = transpose2(inputs={X=['reshape2_9.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_10.tmp_0'], XShape=['reshape2_10.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_14.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_10.tmp_0'], XShape=['transpose_10.tmp_1']} = transpose2(inputs={X=['reshape2_10.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['scale_3.tmp_0']} = scale(inputs={ScaleTensor=[], X=['transpose_8.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 0.125)
    {Out=['matmul_5.tmp_0']} = matmul(inputs={X=['scale_3.tmp_0'], Y=['transpose_9.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = True, use_mkldnn = False, use_quantizer = False)
    {Out=['tmp_9']} = elementwise_add(inputs={X=['matmul_5.tmp_0'], Y=['stack_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['softmax_2.tmp_0']} = softmax(inputs={X=['tmp_9']}, axis = -1, data_format = AnyLayout, is_test = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {Out=['matmul_6.tmp_0']} = matmul(inputs={X=['softmax_2.tmp_0'], Y=['transpose_10.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {Out=['transpose_11.tmp_0'], XShape=['transpose_11.tmp_1']} = transpose2(inputs={X=['matmul_6.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['reshape2_11.tmp_0'], XShape=['reshape2_11.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_11.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 768], use_quantizer = False)
    {Out=['fc_15.tmp_0']} = mul(inputs={X=['reshape2_11.tmp_0'], Y=['encoder_layer_2_multi_head_att_output_fc.w_0']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['fc_15.tmp_1']} = elementwise_add(inputs={X=['fc_15.tmp_0'], Y=['encoder_layer_2_multi_head_att_output_fc.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_10']} = elementwise_add(inputs={X=['fc_15.tmp_1'], Y=['layer_norm_4.tmp_2']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_5.tmp_0'], Variance=['layer_norm_5.tmp_1'], Y=['layer_norm_5.tmp_2']} = layer_norm(inputs={Bias=['encoder_layer_2_post_att_layer_norm_bias'], Scale=['encoder_layer_2_post_att_layer_norm_scale'], X=['tmp_10']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = /, op_role = 0, op_role_var = [])
    {Out=['fc_16.tmp_0']} = mul(inputs={X=['layer_norm_5.tmp_2'], Y=['encoder_layer_2_ffn_fc_0.w_0']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['fc_16.tmp_1']} = elementwise_add(inputs={X=['fc_16.tmp_0'], Y=['encoder_layer_2_ffn_fc_0.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['fc_16.tmp_2']} = gelu(inputs={X=['fc_16.tmp_1']}, approximate = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {Out=['fc_17.tmp_0']} = mul(inputs={X=['fc_16.tmp_2'], Y=['encoder_layer_2_ffn_fc_1.w_0']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['fc_17.tmp_1']} = elementwise_add(inputs={X=['fc_17.tmp_0'], Y=['encoder_layer_2_ffn_fc_1.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_11']} = elementwise_add(inputs={X=['fc_17.tmp_1'], Y=['layer_norm_5.tmp_2']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_6.tmp_0'], Variance=['layer_norm_6.tmp_1'], Y=['layer_norm_6.tmp_2']} = layer_norm(inputs={Bias=['encoder_layer_2_post_ffn_layer_norm_bias'], Scale=['encoder_layer_2_post_ffn_layer_norm_scale'], X=['tmp_11']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = /, op_role = 0, op_role_var = [])
    {Out=['slice_0.tmp_0']} = slice(inputs={EndsTensor=[], EndsTensorList=[], Input=['layer_norm_6.tmp_2'], StartsTensor=[], StartsTensorList=[]}, axes = [1], decrease_axis = [], ends = [1], infer_flags = [1], op_device = , op_namescope = /, op_role = 0, op_role_var = [], starts = [0])
    {Out=['fc_18.tmp_0']} = mul(inputs={X=['slice_0.tmp_0'], Y=['pooled_fc.w_0']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 1, y_num_col_dims = 1)
    {Out=['fc_18.tmp_1']} = elementwise_add(inputs={X=['fc_18.tmp_0'], Y=['pooled_fc.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['fc_18.tmp_2']} = tanh(inputs={X=['fc_18.tmp_1']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {Out=['reshape2_12.tmp_0'], XShape=['reshape2_12.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['layer_norm_6.tmp_2']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [-1, 768], use_quantizer = False)
    {Out=['gather_0.tmp_0']} = gather(inputs={Axis=[], Index=['mask_pos'], X=['reshape2_12.tmp_0']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], overwrite = True)
    {Out=['fc_19.tmp_0']} = mul(inputs={X=['gather_0.tmp_0'], Y=['mask_lm_trans_fc.w_0']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 1, y_num_col_dims = 1)
    {Out=['fc_19.tmp_1']} = elementwise_add(inputs={X=['fc_19.tmp_0'], Y=['mask_lm_trans_fc.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['fc_19.tmp_2']} = gelu(inputs={X=['fc_19.tmp_1']}, approximate = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {Mean=['layer_norm_7.tmp_0'], Variance=['layer_norm_7.tmp_1'], Y=['layer_norm_7.tmp_2']} = layer_norm(inputs={Bias=['mask_lm_trans_layer_norm_bias'], Scale=['mask_lm_trans_layer_norm_scale'], X=['fc_19.tmp_2']}, begin_norm_axis = 1, epsilon = 9.999999747378752e-06, op_device = , op_namescope = /, op_role = 0, op_role_var = [])
    {Out=['matmul_7.tmp_0']} = matmul(inputs={X=['layer_norm_7.tmp_2'], Y=['word_embedding']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], transpose_X = False, transpose_Y = True, use_mkldnn = False, use_quantizer = False)
    {Out=['tmp_12']} = elementwise_add(inputs={X=['matmul_7.tmp_0'], Y=['mask_lm_out_fc.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Loss=['softmax_with_cross_entropy_0.tmp_1'], Softmax=['softmax_with_cross_entropy_0.tmp_0']} = softmax_with_cross_entropy(inputs={Label=['mask_label'], Logits=['tmp_12']}, axis = -1, ignore_index = -100, numeric_stable_mode = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], soft_label = False)
    {Out=['mean_0.tmp_0']} = mean(inputs={X=['softmax_with_cross_entropy_0.tmp_1']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [])
    {Out=['tmp_13']} = elementwise_mul(inputs={X=['mean_0.tmp_0'], Y=['lm_weight']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_14']} = scale(inputs={ScaleTensor=[], X=['word_embedding']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0)
    {Out=['tmp_15']} = scale(inputs={ScaleTensor=[], X=['pos_embedding']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0)
    {Out=['tmp_16']} = scale(inputs={ScaleTensor=[], X=['sent_embedding']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0)
    {Out=['tmp_17']} = scale(inputs={ScaleTensor=[], X=['task_embedding']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0)
    {Out=['tmp_18']} = scale(inputs={ScaleTensor=[], X=['pre_encoder_layer_norm_scale']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0)
    {Out=['tmp_19']} = scale(inputs={ScaleTensor=[], X=['pre_encoder_layer_norm_bias']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0)
    {Out=['tmp_20']} = scale(inputs={ScaleTensor=[], X=['encoder_layer_0_multi_head_att_query_fc.w_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0)
    {Out=['tmp_21']} = scale(inputs={ScaleTensor=[], X=['encoder_layer_0_multi_head_att_query_fc.b_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0)
    {Out=['tmp_22']} = scale(inputs={ScaleTensor=[], X=['encoder_layer_0_multi_head_att_key_fc.w_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0)
    {Out=['tmp_23']} = scale(inputs={ScaleTensor=[], X=['encoder_layer_0_multi_head_att_key_fc.b_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0)
    {Out=['tmp_24']} = scale(inputs={ScaleTensor=[], X=['encoder_layer_0_multi_head_att_value_fc.w_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0)
    {Out=['tmp_25']} = scale(inputs={ScaleTensor=[], X=['encoder_layer_0_multi_head_att_value_fc.b_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0)
    {Out=['tmp_26']} = scale(inputs={ScaleTensor=[], X=['encoder_layer_0_multi_head_att_output_fc.w_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0)
    {Out=['tmp_27']} = scale(inputs={ScaleTensor=[], X=['encoder_layer_0_multi_head_att_output_fc.b_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0)
    {Out=['tmp_28']} = scale(inputs={ScaleTensor=[], X=['encoder_layer_0_post_att_layer_norm_scale']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0)
    {Out=['tmp_29']} = scale(inputs={ScaleTensor=[], X=['encoder_layer_0_post_att_layer_norm_bias']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0)
    {Out=['tmp_30']} = scale(inputs={ScaleTensor=[], X=['encoder_layer_0_ffn_fc_0.w_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0)
    {Out=['tmp_31']} = scale(inputs={ScaleTensor=[], X=['encoder_layer_0_ffn_fc_0.b_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0)
    {Out=['tmp_32']} = scale(inputs={ScaleTensor=[], X=['encoder_layer_0_ffn_fc_1.w_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0)
    {Out=['tmp_33']} = scale(inputs={ScaleTensor=[], X=['encoder_layer_0_ffn_fc_1.b_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0)
    {Out=['tmp_34']} = scale(inputs={ScaleTensor=[], X=['encoder_layer_0_post_ffn_layer_norm_scale']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0)
    {Out=['tmp_35']} = scale(inputs={ScaleTensor=[], X=['encoder_layer_0_post_ffn_layer_norm_bias']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0)
    {Out=['tmp_36']} = scale(inputs={ScaleTensor=[], X=['encoder_layer_1_multi_head_att_query_fc.w_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0)
    {Out=['tmp_37']} = scale(inputs={ScaleTensor=[], X=['encoder_layer_1_multi_head_att_query_fc.b_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0)
    {Out=['tmp_38']} = scale(inputs={ScaleTensor=[], X=['encoder_layer_1_multi_head_att_key_fc.w_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0)
    {Out=['tmp_39']} = scale(inputs={ScaleTensor=[], X=['encoder_layer_1_multi_head_att_key_fc.b_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0)
    {Out=['tmp_40']} = scale(inputs={ScaleTensor=[], X=['encoder_layer_1_multi_head_att_value_fc.w_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0)
    {Out=['tmp_41']} = scale(inputs={ScaleTensor=[], X=['encoder_layer_1_multi_head_att_value_fc.b_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0)
    {Out=['tmp_42']} = scale(inputs={ScaleTensor=[], X=['encoder_layer_1_multi_head_att_output_fc.w_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0)
    {Out=['tmp_43']} = scale(inputs={ScaleTensor=[], X=['encoder_layer_1_multi_head_att_output_fc.b_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0)
    {Out=['tmp_44']} = scale(inputs={ScaleTensor=[], X=['encoder_layer_1_post_att_layer_norm_scale']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0)
    {Out=['tmp_45']} = scale(inputs={ScaleTensor=[], X=['encoder_layer_1_post_att_layer_norm_bias']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0)
    {Out=['tmp_46']} = scale(inputs={ScaleTensor=[], X=['encoder_layer_1_ffn_fc_0.w_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0)
    {Out=['tmp_47']} = scale(inputs={ScaleTensor=[], X=['encoder_layer_1_ffn_fc_0.b_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0)
    {Out=['tmp_48']} = scale(inputs={ScaleTensor=[], X=['encoder_layer_1_ffn_fc_1.w_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0)
    {Out=['tmp_49']} = scale(inputs={ScaleTensor=[], X=['encoder_layer_1_ffn_fc_1.b_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0)
    {Out=['tmp_50']} = scale(inputs={ScaleTensor=[], X=['encoder_layer_1_post_ffn_layer_norm_scale']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0)
    {Out=['tmp_51']} = scale(inputs={ScaleTensor=[], X=['encoder_layer_1_post_ffn_layer_norm_bias']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0)
    {Out=['tmp_52']} = scale(inputs={ScaleTensor=[], X=['encoder_layer_2_multi_head_att_query_fc.w_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0)
    {Out=['tmp_53']} = scale(inputs={ScaleTensor=[], X=['encoder_layer_2_multi_head_att_query_fc.b_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0)
    {Out=['tmp_54']} = scale(inputs={ScaleTensor=[], X=['encoder_layer_2_multi_head_att_key_fc.w_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0)
    {Out=['tmp_55']} = scale(inputs={ScaleTensor=[], X=['encoder_layer_2_multi_head_att_key_fc.b_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0)
    {Out=['tmp_56']} = scale(inputs={ScaleTensor=[], X=['encoder_layer_2_multi_head_att_value_fc.w_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0)
    {Out=['tmp_57']} = scale(inputs={ScaleTensor=[], X=['encoder_layer_2_multi_head_att_value_fc.b_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0)
    {Out=['tmp_58']} = scale(inputs={ScaleTensor=[], X=['encoder_layer_2_multi_head_att_output_fc.w_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0)
    {Out=['tmp_59']} = scale(inputs={ScaleTensor=[], X=['encoder_layer_2_multi_head_att_output_fc.b_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0)
    {Out=['tmp_60']} = scale(inputs={ScaleTensor=[], X=['encoder_layer_2_post_att_layer_norm_scale']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0)
    {Out=['tmp_61']} = scale(inputs={ScaleTensor=[], X=['encoder_layer_2_post_att_layer_norm_bias']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0)
    {Out=['tmp_62']} = scale(inputs={ScaleTensor=[], X=['encoder_layer_2_ffn_fc_0.w_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0)
    {Out=['tmp_63']} = scale(inputs={ScaleTensor=[], X=['encoder_layer_2_ffn_fc_0.b_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0)
    {Out=['tmp_64']} = scale(inputs={ScaleTensor=[], X=['encoder_layer_2_ffn_fc_1.w_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0)
    {Out=['tmp_65']} = scale(inputs={ScaleTensor=[], X=['encoder_layer_2_ffn_fc_1.b_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0)
    {Out=['tmp_66']} = scale(inputs={ScaleTensor=[], X=['encoder_layer_2_post_ffn_layer_norm_scale']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0)
    {Out=['tmp_67']} = scale(inputs={ScaleTensor=[], X=['encoder_layer_2_post_ffn_layer_norm_bias']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0)
    {Out=['tmp_68']} = scale(inputs={ScaleTensor=[], X=['pooled_fc.w_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0)
    {Out=['tmp_69']} = scale(inputs={ScaleTensor=[], X=['pooled_fc.b_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0)
    {Out=['tmp_70']} = scale(inputs={ScaleTensor=[], X=['mask_lm_trans_fc.w_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0)
    {Out=['tmp_71']} = scale(inputs={ScaleTensor=[], X=['mask_lm_trans_fc.b_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0)
    {Out=['tmp_72']} = scale(inputs={ScaleTensor=[], X=['mask_lm_trans_layer_norm_scale']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0)
    {Out=['tmp_73']} = scale(inputs={ScaleTensor=[], X=['mask_lm_trans_layer_norm_bias']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0)
    {Out=['tmp_74']} = scale(inputs={ScaleTensor=[], X=['mask_lm_out_fc.b_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0)
    {Out=['mean_1.tmp_0']} = mean(inputs={X=['tmp_13']}, op_device = , op_namescope = /, op_role = 256, op_role_var = [])
    {Out=['mean_1.tmp_0@GRAD']} = fill_constant(inputs={}, dtype = 5, force_cpu = False, op_device = , op_role = 257, shape = [1], value = 1.0)
    {X@GRAD=['tmp_13@GRAD']} = mean_grad(inputs={Out@GRAD=['mean_1.tmp_0@GRAD'], X=['tmp_13']}, op_device = , op_role = 1)
    {X@GRAD=['mean_0.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_13@GRAD'], X=['mean_0.tmp_0'], Y=['lm_weight']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['softmax_with_cross_entropy_0.tmp_1@GRAD']} = mean_grad(inputs={Out@GRAD=['mean_0.tmp_0@GRAD'], X=['softmax_with_cross_entropy_0.tmp_1']}, op_device = , op_role = 1)
    {Logits@GRAD=['tmp_12@GRAD']} = softmax_with_cross_entropy_grad(inputs={Label=['mask_label'], Loss@GRAD=['softmax_with_cross_entropy_0.tmp_1@GRAD'], Softmax=['softmax_with_cross_entropy_0.tmp_0']}, axis = -1, ignore_index = -100, numeric_stable_mode = True, op_device = , op_namescope = /, op_role = 1, op_role_var = [], soft_label = False)
    {X@GRAD=['matmul_7.tmp_0@GRAD'], Y@GRAD=['mask_lm_out_fc.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_12@GRAD'], X=['matmul_7.tmp_0'], Y=['mask_lm_out_fc.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['mask_lm_out_fc.b_0', 'mask_lm_out_fc.b_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_7.tmp_2@GRAD'], Y@GRAD=['word_embedding@GRAD@RENAME@block0@0']} = matmul_grad(inputs={Out@GRAD=['matmul_7.tmp_0@GRAD'], X=['layer_norm_7.tmp_2'], Y=['word_embedding']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = True, use_mkldnn = False, use_quantizer = False)
    {Bias@GRAD=['mask_lm_trans_layer_norm_bias@GRAD'], Scale@GRAD=['mask_lm_trans_layer_norm_scale@GRAD'], X@GRAD=['fc_19.tmp_2@GRAD']} = layer_norm_grad(inputs={Bias=['mask_lm_trans_layer_norm_bias'], Mean=['layer_norm_7.tmp_0'], Scale=['mask_lm_trans_layer_norm_scale'], Variance=['layer_norm_7.tmp_1'], X=['fc_19.tmp_2'], Y@GRAD=['layer_norm_7.tmp_2@GRAD']}, begin_norm_axis = 1, epsilon = 9.999999747378752e-06, op_device = , op_namescope = /, op_role = 1, op_role_var = ['mask_lm_trans_layer_norm_bias', 'mask_lm_trans_layer_norm_bias@GRAD', 'mask_lm_trans_layer_norm_scale', 'mask_lm_trans_layer_norm_scale@GRAD'])
    {X@GRAD=['fc_19.tmp_1@GRAD']} = gelu_grad(inputs={Out@GRAD=['fc_19.tmp_2@GRAD'], X=['fc_19.tmp_1']}, approximate = False, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {X@GRAD=['fc_19.tmp_0@GRAD'], Y@GRAD=['mask_lm_trans_fc.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_19.tmp_1@GRAD'], X=['fc_19.tmp_0'], Y=['mask_lm_trans_fc.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['mask_lm_trans_fc.b_0', 'mask_lm_trans_fc.b_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['gather_0.tmp_0@GRAD'], Y@GRAD=['mask_lm_trans_fc.w_0@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_19.tmp_0@GRAD'], X=['gather_0.tmp_0'], Y=['mask_lm_trans_fc.w_0']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['mask_lm_trans_fc.w_0', 'mask_lm_trans_fc.w_0@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 1, y_num_col_dims = 1)
    {X@GRAD=['reshape2_12.tmp_0@GRAD']} = gather_grad(inputs={Axis=[], Index=['mask_pos'], Out@GRAD=['gather_0.tmp_0@GRAD'], X=['reshape2_12.tmp_0']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], overwrite = True)
    {X@GRAD=['layer_norm_6.tmp_2@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_12.tmp_0@GRAD'], XShape=['reshape2_12.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [-1, 768], use_quantizer = False)
    {Bias@GRAD=['encoder_layer_2_post_ffn_layer_norm_bias@GRAD'], Scale@GRAD=['encoder_layer_2_post_ffn_layer_norm_scale@GRAD'], X@GRAD=['tmp_11@GRAD']} = layer_norm_grad(inputs={Bias=['encoder_layer_2_post_ffn_layer_norm_bias'], Mean=['layer_norm_6.tmp_0'], Scale=['encoder_layer_2_post_ffn_layer_norm_scale'], Variance=['layer_norm_6.tmp_1'], X=['tmp_11'], Y@GRAD=['layer_norm_6.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_2_post_ffn_layer_norm_bias', 'encoder_layer_2_post_ffn_layer_norm_bias@GRAD', 'encoder_layer_2_post_ffn_layer_norm_scale', 'encoder_layer_2_post_ffn_layer_norm_scale@GRAD'])
    {X@GRAD=['fc_17.tmp_1@GRAD'], Y@GRAD=['layer_norm_5.tmp_2@GRAD@RENAME@block0@0']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_11@GRAD'], X=['fc_17.tmp_1'], Y=['layer_norm_5.tmp_2']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['fc_17.tmp_0@GRAD'], Y@GRAD=['encoder_layer_2_ffn_fc_1.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_17.tmp_1@GRAD'], X=['fc_17.tmp_0'], Y=['encoder_layer_2_ffn_fc_1.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_2_ffn_fc_1.b_0', 'encoder_layer_2_ffn_fc_1.b_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['fc_16.tmp_2@GRAD'], Y@GRAD=['encoder_layer_2_ffn_fc_1.w_0@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_17.tmp_0@GRAD'], X=['fc_16.tmp_2'], Y=['encoder_layer_2_ffn_fc_1.w_0']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_2_ffn_fc_1.w_0', 'encoder_layer_2_ffn_fc_1.w_0@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {X@GRAD=['fc_16.tmp_1@GRAD']} = gelu_grad(inputs={Out@GRAD=['fc_16.tmp_2@GRAD'], X=['fc_16.tmp_1']}, approximate = False, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {X@GRAD=['fc_16.tmp_0@GRAD'], Y@GRAD=['encoder_layer_2_ffn_fc_0.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_16.tmp_1@GRAD'], X=['fc_16.tmp_0'], Y=['encoder_layer_2_ffn_fc_0.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_2_ffn_fc_0.b_0', 'encoder_layer_2_ffn_fc_0.b_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_5.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['encoder_layer_2_ffn_fc_0.w_0@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_16.tmp_0@GRAD'], X=['layer_norm_5.tmp_2'], Y=['encoder_layer_2_ffn_fc_0.w_0']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_2_ffn_fc_0.w_0', 'encoder_layer_2_ffn_fc_0.w_0@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['layer_norm_5.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_5.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_5.tmp_2@GRAD@RENAME@block0@1']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=['encoder_layer_2_post_att_layer_norm_bias@GRAD'], Scale@GRAD=['encoder_layer_2_post_att_layer_norm_scale@GRAD'], X@GRAD=['tmp_10@GRAD']} = layer_norm_grad(inputs={Bias=['encoder_layer_2_post_att_layer_norm_bias'], Mean=['layer_norm_5.tmp_0'], Scale=['encoder_layer_2_post_att_layer_norm_scale'], Variance=['layer_norm_5.tmp_1'], X=['tmp_10'], Y@GRAD=['layer_norm_5.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_2_post_att_layer_norm_bias', 'encoder_layer_2_post_att_layer_norm_bias@GRAD', 'encoder_layer_2_post_att_layer_norm_scale', 'encoder_layer_2_post_att_layer_norm_scale@GRAD'])
    {X@GRAD=['fc_15.tmp_1@GRAD'], Y@GRAD=['layer_norm_4.tmp_2@GRAD@RENAME@block0@0']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_10@GRAD'], X=['fc_15.tmp_1'], Y=['layer_norm_4.tmp_2']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['fc_15.tmp_0@GRAD'], Y@GRAD=['encoder_layer_2_multi_head_att_output_fc.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_15.tmp_1@GRAD'], X=['fc_15.tmp_0'], Y=['encoder_layer_2_multi_head_att_output_fc.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_2_multi_head_att_output_fc.b_0', 'encoder_layer_2_multi_head_att_output_fc.b_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['reshape2_11.tmp_0@GRAD'], Y@GRAD=['encoder_layer_2_multi_head_att_output_fc.w_0@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_15.tmp_0@GRAD'], X=['reshape2_11.tmp_0'], Y=['encoder_layer_2_multi_head_att_output_fc.w_0']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_2_multi_head_att_output_fc.w_0', 'encoder_layer_2_multi_head_att_output_fc.w_0@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {X@GRAD=['transpose_11.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_11.tmp_0@GRAD'], XShape=['reshape2_11.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 768], use_quantizer = False)
    {X@GRAD=['matmul_6.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_11.tmp_0@GRAD'], XShape=['transpose_11.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['softmax_2.tmp_0@GRAD'], Y@GRAD=['transpose_10.tmp_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['matmul_6.tmp_0@GRAD'], X=['softmax_2.tmp_0'], Y=['transpose_10.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['tmp_9@GRAD']} = softmax_grad(inputs={Out=['softmax_2.tmp_0'], Out@GRAD=['softmax_2.tmp_0@GRAD']}, axis = -1, data_format = AnyLayout, is_test = False, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {X@GRAD=['matmul_5.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_add_grad(inputs={Out@GRAD=['tmp_9@GRAD'], X=['matmul_5.tmp_0'], Y=['stack_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['scale_3.tmp_0@GRAD'], Y@GRAD=['transpose_9.tmp_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['matmul_5.tmp_0@GRAD'], X=['scale_3.tmp_0'], Y=['transpose_9.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = True, use_mkldnn = False, use_quantizer = False)
    {Out=['transpose_8.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['scale_3.tmp_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_role = 1, scale = 0.125)
    {X@GRAD=['reshape2_10.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_10.tmp_0@GRAD'], XShape=['transpose_10.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_14.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_10.tmp_0@GRAD'], XShape=['reshape2_10.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['reshape2_9.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_9.tmp_0@GRAD'], XShape=['transpose_9.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_13.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_9.tmp_0@GRAD'], XShape=['reshape2_9.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['reshape2_8.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_8.tmp_0@GRAD'], XShape=['transpose_8.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_12.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_8.tmp_0@GRAD'], XShape=['reshape2_8.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['fc_14.tmp_0@GRAD'], Y@GRAD=['encoder_layer_2_multi_head_att_value_fc.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_14.tmp_1@GRAD'], X=['fc_14.tmp_0'], Y=['encoder_layer_2_multi_head_att_value_fc.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_2_multi_head_att_value_fc.b_0', 'encoder_layer_2_multi_head_att_value_fc.b_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_4.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['encoder_layer_2_multi_head_att_value_fc.w_0@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_14.tmp_0@GRAD'], X=['layer_norm_4.tmp_2'], Y=['encoder_layer_2_multi_head_att_value_fc.w_0']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_2_multi_head_att_value_fc.w_0', 'encoder_layer_2_multi_head_att_value_fc.w_0@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {X@GRAD=['fc_13.tmp_0@GRAD'], Y@GRAD=['encoder_layer_2_multi_head_att_key_fc.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_13.tmp_1@GRAD'], X=['fc_13.tmp_0'], Y=['encoder_layer_2_multi_head_att_key_fc.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_2_multi_head_att_key_fc.b_0', 'encoder_layer_2_multi_head_att_key_fc.b_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_4.tmp_2@GRAD@RENAME@block0@2'], Y@GRAD=['encoder_layer_2_multi_head_att_key_fc.w_0@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_13.tmp_0@GRAD'], X=['layer_norm_4.tmp_2'], Y=['encoder_layer_2_multi_head_att_key_fc.w_0']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_2_multi_head_att_key_fc.w_0', 'encoder_layer_2_multi_head_att_key_fc.w_0@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {X@GRAD=['fc_12.tmp_0@GRAD'], Y@GRAD=['encoder_layer_2_multi_head_att_query_fc.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_12.tmp_1@GRAD'], X=['fc_12.tmp_0'], Y=['encoder_layer_2_multi_head_att_query_fc.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_2_multi_head_att_query_fc.b_0', 'encoder_layer_2_multi_head_att_query_fc.b_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_4.tmp_2@GRAD@RENAME@block0@3'], Y@GRAD=['encoder_layer_2_multi_head_att_query_fc.w_0@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_12.tmp_0@GRAD'], X=['layer_norm_4.tmp_2'], Y=['encoder_layer_2_multi_head_att_query_fc.w_0']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_2_multi_head_att_query_fc.w_0', 'encoder_layer_2_multi_head_att_query_fc.w_0@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['layer_norm_4.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_4.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_4.tmp_2@GRAD@RENAME@block0@1', 'layer_norm_4.tmp_2@GRAD@RENAME@block0@2', 'layer_norm_4.tmp_2@GRAD@RENAME@block0@3']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=['encoder_layer_1_post_ffn_layer_norm_bias@GRAD'], Scale@GRAD=['encoder_layer_1_post_ffn_layer_norm_scale@GRAD'], X@GRAD=['tmp_8@GRAD']} = layer_norm_grad(inputs={Bias=['encoder_layer_1_post_ffn_layer_norm_bias'], Mean=['layer_norm_4.tmp_0'], Scale=['encoder_layer_1_post_ffn_layer_norm_scale'], Variance=['layer_norm_4.tmp_1'], X=['tmp_8'], Y@GRAD=['layer_norm_4.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_1_post_ffn_layer_norm_bias', 'encoder_layer_1_post_ffn_layer_norm_bias@GRAD', 'encoder_layer_1_post_ffn_layer_norm_scale', 'encoder_layer_1_post_ffn_layer_norm_scale@GRAD'])
    {X@GRAD=['fc_11.tmp_1@GRAD'], Y@GRAD=['layer_norm_3.tmp_2@GRAD@RENAME@block0@0']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_8@GRAD'], X=['fc_11.tmp_1'], Y=['layer_norm_3.tmp_2']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['fc_11.tmp_0@GRAD'], Y@GRAD=['encoder_layer_1_ffn_fc_1.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_11.tmp_1@GRAD'], X=['fc_11.tmp_0'], Y=['encoder_layer_1_ffn_fc_1.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_1_ffn_fc_1.b_0', 'encoder_layer_1_ffn_fc_1.b_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['fc_10.tmp_2@GRAD'], Y@GRAD=['encoder_layer_1_ffn_fc_1.w_0@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_11.tmp_0@GRAD'], X=['fc_10.tmp_2'], Y=['encoder_layer_1_ffn_fc_1.w_0']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_1_ffn_fc_1.w_0', 'encoder_layer_1_ffn_fc_1.w_0@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {X@GRAD=['fc_10.tmp_1@GRAD']} = gelu_grad(inputs={Out@GRAD=['fc_10.tmp_2@GRAD'], X=['fc_10.tmp_1']}, approximate = False, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {X@GRAD=['fc_10.tmp_0@GRAD'], Y@GRAD=['encoder_layer_1_ffn_fc_0.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_10.tmp_1@GRAD'], X=['fc_10.tmp_0'], Y=['encoder_layer_1_ffn_fc_0.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_1_ffn_fc_0.b_0', 'encoder_layer_1_ffn_fc_0.b_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_3.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['encoder_layer_1_ffn_fc_0.w_0@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_10.tmp_0@GRAD'], X=['layer_norm_3.tmp_2'], Y=['encoder_layer_1_ffn_fc_0.w_0']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_1_ffn_fc_0.w_0', 'encoder_layer_1_ffn_fc_0.w_0@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['layer_norm_3.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_3.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_3.tmp_2@GRAD@RENAME@block0@1']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=['encoder_layer_1_post_att_layer_norm_bias@GRAD'], Scale@GRAD=['encoder_layer_1_post_att_layer_norm_scale@GRAD'], X@GRAD=['tmp_7@GRAD']} = layer_norm_grad(inputs={Bias=['encoder_layer_1_post_att_layer_norm_bias'], Mean=['layer_norm_3.tmp_0'], Scale=['encoder_layer_1_post_att_layer_norm_scale'], Variance=['layer_norm_3.tmp_1'], X=['tmp_7'], Y@GRAD=['layer_norm_3.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_1_post_att_layer_norm_bias', 'encoder_layer_1_post_att_layer_norm_bias@GRAD', 'encoder_layer_1_post_att_layer_norm_scale', 'encoder_layer_1_post_att_layer_norm_scale@GRAD'])
    {X@GRAD=['fc_9.tmp_1@GRAD'], Y@GRAD=['layer_norm_2.tmp_2@GRAD@RENAME@block0@0']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_7@GRAD'], X=['fc_9.tmp_1'], Y=['layer_norm_2.tmp_2']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['fc_9.tmp_0@GRAD'], Y@GRAD=['encoder_layer_1_multi_head_att_output_fc.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_9.tmp_1@GRAD'], X=['fc_9.tmp_0'], Y=['encoder_layer_1_multi_head_att_output_fc.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_1_multi_head_att_output_fc.b_0', 'encoder_layer_1_multi_head_att_output_fc.b_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['reshape2_7.tmp_0@GRAD'], Y@GRAD=['encoder_layer_1_multi_head_att_output_fc.w_0@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_9.tmp_0@GRAD'], X=['reshape2_7.tmp_0'], Y=['encoder_layer_1_multi_head_att_output_fc.w_0']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_1_multi_head_att_output_fc.w_0', 'encoder_layer_1_multi_head_att_output_fc.w_0@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {X@GRAD=['transpose_7.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_7.tmp_0@GRAD'], XShape=['reshape2_7.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 768], use_quantizer = False)
    {X@GRAD=['matmul_4.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_7.tmp_0@GRAD'], XShape=['transpose_7.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['softmax_1.tmp_0@GRAD'], Y@GRAD=['transpose_6.tmp_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['matmul_4.tmp_0@GRAD'], X=['softmax_1.tmp_0'], Y=['transpose_6.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['tmp_6@GRAD']} = softmax_grad(inputs={Out=['softmax_1.tmp_0'], Out@GRAD=['softmax_1.tmp_0@GRAD']}, axis = -1, data_format = AnyLayout, is_test = False, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {X@GRAD=['matmul_3.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_add_grad(inputs={Out@GRAD=['tmp_6@GRAD'], X=['matmul_3.tmp_0'], Y=['stack_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['scale_2.tmp_0@GRAD'], Y@GRAD=['transpose_5.tmp_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['matmul_3.tmp_0@GRAD'], X=['scale_2.tmp_0'], Y=['transpose_5.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = True, use_mkldnn = False, use_quantizer = False)
    {Out=['transpose_4.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['scale_2.tmp_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_role = 1, scale = 0.125)
    {X@GRAD=['reshape2_6.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_6.tmp_0@GRAD'], XShape=['transpose_6.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_8.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_6.tmp_0@GRAD'], XShape=['reshape2_6.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['reshape2_5.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_5.tmp_0@GRAD'], XShape=['transpose_5.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_7.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_5.tmp_0@GRAD'], XShape=['reshape2_5.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['reshape2_4.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_4.tmp_0@GRAD'], XShape=['transpose_4.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_6.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_4.tmp_0@GRAD'], XShape=['reshape2_4.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['fc_8.tmp_0@GRAD'], Y@GRAD=['encoder_layer_1_multi_head_att_value_fc.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_8.tmp_1@GRAD'], X=['fc_8.tmp_0'], Y=['encoder_layer_1_multi_head_att_value_fc.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_1_multi_head_att_value_fc.b_0', 'encoder_layer_1_multi_head_att_value_fc.b_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_2.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['encoder_layer_1_multi_head_att_value_fc.w_0@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_8.tmp_0@GRAD'], X=['layer_norm_2.tmp_2'], Y=['encoder_layer_1_multi_head_att_value_fc.w_0']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_1_multi_head_att_value_fc.w_0', 'encoder_layer_1_multi_head_att_value_fc.w_0@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {X@GRAD=['fc_7.tmp_0@GRAD'], Y@GRAD=['encoder_layer_1_multi_head_att_key_fc.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_7.tmp_1@GRAD'], X=['fc_7.tmp_0'], Y=['encoder_layer_1_multi_head_att_key_fc.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_1_multi_head_att_key_fc.b_0', 'encoder_layer_1_multi_head_att_key_fc.b_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_2.tmp_2@GRAD@RENAME@block0@2'], Y@GRAD=['encoder_layer_1_multi_head_att_key_fc.w_0@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_7.tmp_0@GRAD'], X=['layer_norm_2.tmp_2'], Y=['encoder_layer_1_multi_head_att_key_fc.w_0']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_1_multi_head_att_key_fc.w_0', 'encoder_layer_1_multi_head_att_key_fc.w_0@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {X@GRAD=['fc_6.tmp_0@GRAD'], Y@GRAD=['encoder_layer_1_multi_head_att_query_fc.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_6.tmp_1@GRAD'], X=['fc_6.tmp_0'], Y=['encoder_layer_1_multi_head_att_query_fc.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_1_multi_head_att_query_fc.b_0', 'encoder_layer_1_multi_head_att_query_fc.b_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_2.tmp_2@GRAD@RENAME@block0@3'], Y@GRAD=['encoder_layer_1_multi_head_att_query_fc.w_0@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_6.tmp_0@GRAD'], X=['layer_norm_2.tmp_2'], Y=['encoder_layer_1_multi_head_att_query_fc.w_0']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_1_multi_head_att_query_fc.w_0', 'encoder_layer_1_multi_head_att_query_fc.w_0@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['layer_norm_2.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_2.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_2.tmp_2@GRAD@RENAME@block0@1', 'layer_norm_2.tmp_2@GRAD@RENAME@block0@2', 'layer_norm_2.tmp_2@GRAD@RENAME@block0@3']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=['encoder_layer_0_post_ffn_layer_norm_bias@GRAD'], Scale@GRAD=['encoder_layer_0_post_ffn_layer_norm_scale@GRAD'], X@GRAD=['tmp_5@GRAD']} = layer_norm_grad(inputs={Bias=['encoder_layer_0_post_ffn_layer_norm_bias'], Mean=['layer_norm_2.tmp_0'], Scale=['encoder_layer_0_post_ffn_layer_norm_scale'], Variance=['layer_norm_2.tmp_1'], X=['tmp_5'], Y@GRAD=['layer_norm_2.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_0_post_ffn_layer_norm_bias', 'encoder_layer_0_post_ffn_layer_norm_bias@GRAD', 'encoder_layer_0_post_ffn_layer_norm_scale', 'encoder_layer_0_post_ffn_layer_norm_scale@GRAD'])
    {X@GRAD=['fc_5.tmp_1@GRAD'], Y@GRAD=['layer_norm_1.tmp_2@GRAD@RENAME@block0@0']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_5@GRAD'], X=['fc_5.tmp_1'], Y=['layer_norm_1.tmp_2']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['fc_5.tmp_0@GRAD'], Y@GRAD=['encoder_layer_0_ffn_fc_1.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_5.tmp_1@GRAD'], X=['fc_5.tmp_0'], Y=['encoder_layer_0_ffn_fc_1.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_0_ffn_fc_1.b_0', 'encoder_layer_0_ffn_fc_1.b_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['fc_4.tmp_2@GRAD'], Y@GRAD=['encoder_layer_0_ffn_fc_1.w_0@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_5.tmp_0@GRAD'], X=['fc_4.tmp_2'], Y=['encoder_layer_0_ffn_fc_1.w_0']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_0_ffn_fc_1.w_0', 'encoder_layer_0_ffn_fc_1.w_0@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {X@GRAD=['fc_4.tmp_1@GRAD']} = gelu_grad(inputs={Out@GRAD=['fc_4.tmp_2@GRAD'], X=['fc_4.tmp_1']}, approximate = False, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {X@GRAD=['fc_4.tmp_0@GRAD'], Y@GRAD=['encoder_layer_0_ffn_fc_0.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_4.tmp_1@GRAD'], X=['fc_4.tmp_0'], Y=['encoder_layer_0_ffn_fc_0.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_0_ffn_fc_0.b_0', 'encoder_layer_0_ffn_fc_0.b_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_1.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['encoder_layer_0_ffn_fc_0.w_0@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_4.tmp_0@GRAD'], X=['layer_norm_1.tmp_2'], Y=['encoder_layer_0_ffn_fc_0.w_0']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_0_ffn_fc_0.w_0', 'encoder_layer_0_ffn_fc_0.w_0@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['layer_norm_1.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_1.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_1.tmp_2@GRAD@RENAME@block0@1']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=['encoder_layer_0_post_att_layer_norm_bias@GRAD'], Scale@GRAD=['encoder_layer_0_post_att_layer_norm_scale@GRAD'], X@GRAD=['tmp_4@GRAD']} = layer_norm_grad(inputs={Bias=['encoder_layer_0_post_att_layer_norm_bias'], Mean=['layer_norm_1.tmp_0'], Scale=['encoder_layer_0_post_att_layer_norm_scale'], Variance=['layer_norm_1.tmp_1'], X=['tmp_4'], Y@GRAD=['layer_norm_1.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_0_post_att_layer_norm_bias', 'encoder_layer_0_post_att_layer_norm_bias@GRAD', 'encoder_layer_0_post_att_layer_norm_scale', 'encoder_layer_0_post_att_layer_norm_scale@GRAD'])
    {X@GRAD=['fc_3.tmp_1@GRAD'], Y@GRAD=['layer_norm_0.tmp_2@GRAD@RENAME@block0@0']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_4@GRAD'], X=['fc_3.tmp_1'], Y=['layer_norm_0.tmp_2']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['fc_3.tmp_0@GRAD'], Y@GRAD=['encoder_layer_0_multi_head_att_output_fc.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_3.tmp_1@GRAD'], X=['fc_3.tmp_0'], Y=['encoder_layer_0_multi_head_att_output_fc.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_0_multi_head_att_output_fc.b_0', 'encoder_layer_0_multi_head_att_output_fc.b_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['reshape2_3.tmp_0@GRAD'], Y@GRAD=['encoder_layer_0_multi_head_att_output_fc.w_0@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_3.tmp_0@GRAD'], X=['reshape2_3.tmp_0'], Y=['encoder_layer_0_multi_head_att_output_fc.w_0']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_0_multi_head_att_output_fc.w_0', 'encoder_layer_0_multi_head_att_output_fc.w_0@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {X@GRAD=['transpose_3.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_3.tmp_0@GRAD'], XShape=['reshape2_3.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 768], use_quantizer = False)
    {X@GRAD=['matmul_2.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_3.tmp_0@GRAD'], XShape=['transpose_3.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['softmax_0.tmp_0@GRAD'], Y@GRAD=['transpose_2.tmp_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['matmul_2.tmp_0@GRAD'], X=['softmax_0.tmp_0'], Y=['transpose_2.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = False, use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['tmp_3@GRAD']} = softmax_grad(inputs={Out=['softmax_0.tmp_0'], Out@GRAD=['softmax_0.tmp_0@GRAD']}, axis = -1, data_format = AnyLayout, is_test = False, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {X@GRAD=['matmul_1.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_add_grad(inputs={Out@GRAD=['tmp_3@GRAD'], X=['matmul_1.tmp_0'], Y=['stack_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['scale_1.tmp_0@GRAD'], Y@GRAD=['transpose_1.tmp_0@GRAD']} = matmul_grad(inputs={Out@GRAD=['matmul_1.tmp_0@GRAD'], X=['scale_1.tmp_0'], Y=['transpose_1.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, alpha = 1.0, force_fp32_output = False, fused_reshape_Out = [], fused_reshape_X = [], fused_reshape_Y = [], fused_transpose_Out = [], fused_transpose_X = [], fused_transpose_Y = [], mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], transpose_X = False, transpose_Y = True, use_mkldnn = False, use_quantizer = False)
    {Out=['transpose_0.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['scale_1.tmp_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_role = 1, scale = 0.125)
    {X@GRAD=['reshape2_2.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_2.tmp_0@GRAD'], XShape=['transpose_2.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_2.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_2.tmp_0@GRAD'], XShape=['reshape2_2.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['reshape2_1.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_1.tmp_0@GRAD'], XShape=['transpose_1.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_1.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_1.tmp_0@GRAD'], XShape=['reshape2_1.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['reshape2_0.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_0.tmp_0@GRAD'], XShape=['transpose_0.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_0.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_0.tmp_0@GRAD'], XShape=['reshape2_0.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['fc_2.tmp_0@GRAD'], Y@GRAD=['encoder_layer_0_multi_head_att_value_fc.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_2.tmp_1@GRAD'], X=['fc_2.tmp_0'], Y=['encoder_layer_0_multi_head_att_value_fc.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_0_multi_head_att_value_fc.b_0', 'encoder_layer_0_multi_head_att_value_fc.b_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_0.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['encoder_layer_0_multi_head_att_value_fc.w_0@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_2.tmp_0@GRAD'], X=['layer_norm_0.tmp_2'], Y=['encoder_layer_0_multi_head_att_value_fc.w_0']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_0_multi_head_att_value_fc.w_0', 'encoder_layer_0_multi_head_att_value_fc.w_0@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {X@GRAD=['fc_1.tmp_0@GRAD'], Y@GRAD=['encoder_layer_0_multi_head_att_key_fc.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_1.tmp_1@GRAD'], X=['fc_1.tmp_0'], Y=['encoder_layer_0_multi_head_att_key_fc.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_0_multi_head_att_key_fc.b_0', 'encoder_layer_0_multi_head_att_key_fc.b_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_0.tmp_2@GRAD@RENAME@block0@2'], Y@GRAD=['encoder_layer_0_multi_head_att_key_fc.w_0@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_1.tmp_0@GRAD'], X=['layer_norm_0.tmp_2'], Y=['encoder_layer_0_multi_head_att_key_fc.w_0']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_0_multi_head_att_key_fc.w_0', 'encoder_layer_0_multi_head_att_key_fc.w_0@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {X@GRAD=['fc_0.tmp_0@GRAD'], Y@GRAD=['encoder_layer_0_multi_head_att_query_fc.b_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_0.tmp_1@GRAD'], X=['fc_0.tmp_0'], Y=['encoder_layer_0_multi_head_att_query_fc.b_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_0_multi_head_att_query_fc.b_0', 'encoder_layer_0_multi_head_att_query_fc.b_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_0.tmp_2@GRAD@RENAME@block0@3'], Y@GRAD=['encoder_layer_0_multi_head_att_query_fc.w_0@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_0.tmp_0@GRAD'], X=['layer_norm_0.tmp_2'], Y=['encoder_layer_0_multi_head_att_query_fc.w_0']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_0_multi_head_att_query_fc.w_0', 'encoder_layer_0_multi_head_att_query_fc.w_0@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['layer_norm_0.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_0.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_0.tmp_2@GRAD@RENAME@block0@1', 'layer_norm_0.tmp_2@GRAD@RENAME@block0@2', 'layer_norm_0.tmp_2@GRAD@RENAME@block0@3']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=['pre_encoder_layer_norm_bias@GRAD'], Scale@GRAD=['pre_encoder_layer_norm_scale@GRAD'], X@GRAD=['tmp_2@GRAD']} = layer_norm_grad(inputs={Bias=['pre_encoder_layer_norm_bias'], Mean=['layer_norm_0.tmp_0'], Scale=['pre_encoder_layer_norm_scale'], Variance=['layer_norm_0.tmp_1'], X=['tmp_2'], Y@GRAD=['layer_norm_0.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999747378752e-06, op_device = , op_namescope = /, op_role = 1, op_role_var = ['pre_encoder_layer_norm_bias', 'pre_encoder_layer_norm_bias@GRAD', 'pre_encoder_layer_norm_scale', 'pre_encoder_layer_norm_scale@GRAD'])
    {X@GRAD=['tmp_1@GRAD'], Y@GRAD=['embedding_3.tmp_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_2@GRAD'], X=['tmp_1'], Y=['embedding_3.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['tmp_0@GRAD'], Y@GRAD=['embedding_2.tmp_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_1@GRAD'], X=['tmp_0'], Y=['embedding_2.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['embedding_0.tmp_0@GRAD'], Y@GRAD=['embedding_1.tmp_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_0@GRAD'], X=['embedding_0.tmp_0'], Y=['embedding_1.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {W@GRAD=['task_embedding@GRAD']} = lookup_table_grad(inputs={Ids=['task_ids'], Out@GRAD=['embedding_3.tmp_0@GRAD'], W=['task_embedding']}, entry = none, entry_config = , epmap = [], grad_inplace = False, height_sections = [], is_distributed = False, is_sparse = False, is_test = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['task_embedding', 'task_embedding@GRAD'], padding_idx = -1, remote_prefetch = False, table_names = [], trainer_id = 0)
    {W@GRAD=['sent_embedding@GRAD']} = lookup_table_grad(inputs={Ids=['sent_ids'], Out@GRAD=['embedding_2.tmp_0@GRAD'], W=['sent_embedding']}, entry = none, entry_config = , epmap = [], grad_inplace = False, height_sections = [], is_distributed = False, is_sparse = False, is_test = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['sent_embedding', 'sent_embedding@GRAD'], padding_idx = -1, remote_prefetch = False, table_names = [], trainer_id = 0)
    {W@GRAD=['pos_embedding@GRAD']} = lookup_table_grad(inputs={Ids=['pos_ids'], Out@GRAD=['embedding_1.tmp_0@GRAD'], W=['pos_embedding']}, entry = none, entry_config = , epmap = [], grad_inplace = False, height_sections = [], is_distributed = False, is_sparse = False, is_test = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['pos_embedding', 'pos_embedding@GRAD'], padding_idx = -1, remote_prefetch = False, table_names = [], trainer_id = 0)
    {W@GRAD=['word_embedding@GRAD@RENAME@block0@1']} = lookup_table_grad(inputs={Ids=['src_ids'], Out@GRAD=['embedding_0.tmp_0@GRAD'], W=['word_embedding']}, entry = none, entry_config = , epmap = [], grad_inplace = False, height_sections = [], is_distributed = False, is_sparse = False, is_test = False, op_device = , op_namescope = /, op_role = 1, op_role_var = [], padding_idx = -1, remote_prefetch = False, table_names = [], trainer_id = 0)
    {Out=['word_embedding@GRAD']} = sum(inputs={X=['word_embedding@GRAD@RENAME@block0@0', 'word_embedding@GRAD@RENAME@block0@1']}, op_device = , op_role = 1, op_role_var = ['word_embedding', 'word_embedding@GRAD'], use_mkldnn = False)
    {Out=['fill_constant_1.tmp_0']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /gradient_clip_@CLIP/, op_role = 2, op_role_var = ['encoder_layer_0_ffn_fc_0.b_0', 'elementwise_mul_0.tmp_0'], shape = [1], str_value = 1.0, value = 0.0)
    {Out=['square_0.tmp_0']} = square(inputs={X=['encoder_layer_0_ffn_fc_0.b_0@GRAD']}, op_device = , op_namescope = /gradient_clip_@CLIP/, op_role = 2, op_role_var = ['encoder_layer_0_ffn_fc_0.b_0', 'elementwise_mul_0.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_0.tmp_0']} = reduce_sum(inputs={X=['square_0.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip_@CLIP/, op_role = 2, op_role_var = ['encoder_layer_0_ffn_fc_0.b_0', 'elementwise_mul_0.tmp_0'], out_dtype = -1, reduce_all = True)
    {Out=['square_1.tmp_0']} = square(inputs={X=['encoder_layer_0_ffn_fc_0.w_0@GRAD']}, op_device = , op_namescope = /gradient_clip_@CLIP_1/, op_role = 2, op_role_var = ['encoder_layer_0_ffn_fc_0.w_0', 'elementwise_mul_1.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_1.tmp_0']} = reduce_sum(inputs={X=['square_1.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip_@CLIP_1/, op_role = 2, op_role_var = ['encoder_layer_0_ffn_fc_0.w_0', 'elementwise_mul_1.tmp_0'], out_dtype = -1, reduce_all = True)
    {Out=['square_2.tmp_0']} = square(inputs={X=['encoder_layer_0_ffn_fc_1.b_0@GRAD']}, op_device = , op_namescope = /gradient_clip_@CLIP_2/, op_role = 2, op_role_var = ['encoder_layer_0_ffn_fc_1.b_0', 'elementwise_mul_2.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_2.tmp_0']} = reduce_sum(inputs={X=['square_2.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip_@CLIP_2/, op_role = 2, op_role_var = ['encoder_layer_0_ffn_fc_1.b_0', 'elementwise_mul_2.tmp_0'], out_dtype = -1, reduce_all = True)
    {Out=['square_3.tmp_0']} = square(inputs={X=['encoder_layer_0_ffn_fc_1.w_0@GRAD']}, op_device = , op_namescope = /gradient_clip_@CLIP_3/, op_role = 2, op_role_var = ['encoder_layer_0_ffn_fc_1.w_0', 'elementwise_mul_3.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_3.tmp_0']} = reduce_sum(inputs={X=['square_3.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip_@CLIP_3/, op_role = 2, op_role_var = ['encoder_layer_0_ffn_fc_1.w_0', 'elementwise_mul_3.tmp_0'], out_dtype = -1, reduce_all = True)
    {Out=['square_4.tmp_0']} = square(inputs={X=['encoder_layer_0_multi_head_att_key_fc.b_0@GRAD']}, op_device = , op_namescope = /gradient_clip_@CLIP_4/, op_role = 2, op_role_var = ['encoder_layer_0_multi_head_att_key_fc.b_0', 'elementwise_mul_4.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_4.tmp_0']} = reduce_sum(inputs={X=['square_4.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip_@CLIP_4/, op_role = 2, op_role_var = ['encoder_layer_0_multi_head_att_key_fc.b_0', 'elementwise_mul_4.tmp_0'], out_dtype = -1, reduce_all = True)
    {Out=['square_5.tmp_0']} = square(inputs={X=['encoder_layer_0_multi_head_att_key_fc.w_0@GRAD']}, op_device = , op_namescope = /gradient_clip_@CLIP_5/, op_role = 2, op_role_var = ['encoder_layer_0_multi_head_att_key_fc.w_0', 'elementwise_mul_5.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_5.tmp_0']} = reduce_sum(inputs={X=['square_5.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip_@CLIP_5/, op_role = 2, op_role_var = ['encoder_layer_0_multi_head_att_key_fc.w_0', 'elementwise_mul_5.tmp_0'], out_dtype = -1, reduce_all = True)
    {Out=['square_6.tmp_0']} = square(inputs={X=['encoder_layer_0_multi_head_att_output_fc.b_0@GRAD']}, op_device = , op_namescope = /gradient_clip_@CLIP_6/, op_role = 2, op_role_var = ['encoder_layer_0_multi_head_att_output_fc.b_0', 'elementwise_mul_6.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_6.tmp_0']} = reduce_sum(inputs={X=['square_6.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip_@CLIP_6/, op_role = 2, op_role_var = ['encoder_layer_0_multi_head_att_output_fc.b_0', 'elementwise_mul_6.tmp_0'], out_dtype = -1, reduce_all = True)
    {Out=['square_7.tmp_0']} = square(inputs={X=['encoder_layer_0_multi_head_att_output_fc.w_0@GRAD']}, op_device = , op_namescope = /gradient_clip_@CLIP_7/, op_role = 2, op_role_var = ['encoder_layer_0_multi_head_att_output_fc.w_0', 'elementwise_mul_7.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_7.tmp_0']} = reduce_sum(inputs={X=['square_7.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip_@CLIP_7/, op_role = 2, op_role_var = ['encoder_layer_0_multi_head_att_output_fc.w_0', 'elementwise_mul_7.tmp_0'], out_dtype = -1, reduce_all = True)
    {Out=['square_8.tmp_0']} = square(inputs={X=['encoder_layer_0_multi_head_att_query_fc.b_0@GRAD']}, op_device = , op_namescope = /gradient_clip_@CLIP_8/, op_role = 2, op_role_var = ['encoder_layer_0_multi_head_att_query_fc.b_0', 'elementwise_mul_8.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_8.tmp_0']} = reduce_sum(inputs={X=['square_8.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip_@CLIP_8/, op_role = 2, op_role_var = ['encoder_layer_0_multi_head_att_query_fc.b_0', 'elementwise_mul_8.tmp_0'], out_dtype = -1, reduce_all = True)
    {Out=['square_9.tmp_0']} = square(inputs={X=['encoder_layer_0_multi_head_att_query_fc.w_0@GRAD']}, op_device = , op_namescope = /gradient_clip_@CLIP_9/, op_role = 2, op_role_var = ['encoder_layer_0_multi_head_att_query_fc.w_0', 'elementwise_mul_9.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_9.tmp_0']} = reduce_sum(inputs={X=['square_9.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip_@CLIP_9/, op_role = 2, op_role_var = ['encoder_layer_0_multi_head_att_query_fc.w_0', 'elementwise_mul_9.tmp_0'], out_dtype = -1, reduce_all = True)
    {Out=['square_10.tmp_0']} = square(inputs={X=['encoder_layer_0_multi_head_att_value_fc.b_0@GRAD']}, op_device = , op_namescope = /gradient_clip_@CLIP_10/, op_role = 2, op_role_var = ['encoder_layer_0_multi_head_att_value_fc.b_0', 'elementwise_mul_10.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_10.tmp_0']} = reduce_sum(inputs={X=['square_10.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip_@CLIP_10/, op_role = 2, op_role_var = ['encoder_layer_0_multi_head_att_value_fc.b_0', 'elementwise_mul_10.tmp_0'], out_dtype = -1, reduce_all = True)
    {Out=['square_11.tmp_0']} = square(inputs={X=['encoder_layer_0_multi_head_att_value_fc.w_0@GRAD']}, op_device = , op_namescope = /gradient_clip_@CLIP_11/, op_role = 2, op_role_var = ['encoder_layer_0_multi_head_att_value_fc.w_0', 'elementwise_mul_11.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_11.tmp_0']} = reduce_sum(inputs={X=['square_11.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip_@CLIP_11/, op_role = 2, op_role_var = ['encoder_layer_0_multi_head_att_value_fc.w_0', 'elementwise_mul_11.tmp_0'], out_dtype = -1, reduce_all = True)
    {Out=['square_12.tmp_0']} = square(inputs={X=['encoder_layer_0_post_att_layer_norm_bias@GRAD']}, op_device = , op_namescope = /gradient_clip_@CLIP_12/, op_role = 2, op_role_var = ['encoder_layer_0_post_att_layer_norm_bias', 'elementwise_mul_12.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_12.tmp_0']} = reduce_sum(inputs={X=['square_12.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip_@CLIP_12/, op_role = 2, op_role_var = ['encoder_layer_0_post_att_layer_norm_bias', 'elementwise_mul_12.tmp_0'], out_dtype = -1, reduce_all = True)
    {Out=['square_13.tmp_0']} = square(inputs={X=['encoder_layer_0_post_att_layer_norm_scale@GRAD']}, op_device = , op_namescope = /gradient_clip_@CLIP_13/, op_role = 2, op_role_var = ['encoder_layer_0_post_att_layer_norm_scale', 'elementwise_mul_13.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_13.tmp_0']} = reduce_sum(inputs={X=['square_13.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip_@CLIP_13/, op_role = 2, op_role_var = ['encoder_layer_0_post_att_layer_norm_scale', 'elementwise_mul_13.tmp_0'], out_dtype = -1, reduce_all = True)
    {Out=['square_14.tmp_0']} = square(inputs={X=['encoder_layer_0_post_ffn_layer_norm_bias@GRAD']}, op_device = , op_namescope = /gradient_clip_@CLIP_14/, op_role = 2, op_role_var = ['encoder_layer_0_post_ffn_layer_norm_bias', 'elementwise_mul_14.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_14.tmp_0']} = reduce_sum(inputs={X=['square_14.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip_@CLIP_14/, op_role = 2, op_role_var = ['encoder_layer_0_post_ffn_layer_norm_bias', 'elementwise_mul_14.tmp_0'], out_dtype = -1, reduce_all = True)
    {Out=['square_15.tmp_0']} = square(inputs={X=['encoder_layer_0_post_ffn_layer_norm_scale@GRAD']}, op_device = , op_namescope = /gradient_clip_@CLIP_15/, op_role = 2, op_role_var = ['encoder_layer_0_post_ffn_layer_norm_scale', 'elementwise_mul_15.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_15.tmp_0']} = reduce_sum(inputs={X=['square_15.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip_@CLIP_15/, op_role = 2, op_role_var = ['encoder_layer_0_post_ffn_layer_norm_scale', 'elementwise_mul_15.tmp_0'], out_dtype = -1, reduce_all = True)
    {Out=['square_16.tmp_0']} = square(inputs={X=['encoder_layer_1_ffn_fc_0.b_0@GRAD']}, op_device = , op_namescope = /gradient_clip_@CLIP_16/, op_role = 2, op_role_var = ['encoder_layer_1_ffn_fc_0.b_0', 'elementwise_mul_16.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_16.tmp_0']} = reduce_sum(inputs={X=['square_16.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip_@CLIP_16/, op_role = 2, op_role_var = ['encoder_layer_1_ffn_fc_0.b_0', 'elementwise_mul_16.tmp_0'], out_dtype = -1, reduce_all = True)
    {Out=['square_17.tmp_0']} = square(inputs={X=['encoder_layer_1_ffn_fc_0.w_0@GRAD']}, op_device = , op_namescope = /gradient_clip_@CLIP_17/, op_role = 2, op_role_var = ['encoder_layer_1_ffn_fc_0.w_0', 'elementwise_mul_17.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_17.tmp_0']} = reduce_sum(inputs={X=['square_17.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip_@CLIP_17/, op_role = 2, op_role_var = ['encoder_layer_1_ffn_fc_0.w_0', 'elementwise_mul_17.tmp_0'], out_dtype = -1, reduce_all = True)
    {Out=['square_18.tmp_0']} = square(inputs={X=['encoder_layer_1_ffn_fc_1.b_0@GRAD']}, op_device = , op_namescope = /gradient_clip_@CLIP_18/, op_role = 2, op_role_var = ['encoder_layer_1_ffn_fc_1.b_0', 'elementwise_mul_18.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_18.tmp_0']} = reduce_sum(inputs={X=['square_18.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip_@CLIP_18/, op_role = 2, op_role_var = ['encoder_layer_1_ffn_fc_1.b_0', 'elementwise_mul_18.tmp_0'], out_dtype = -1, reduce_all = True)
    {Out=['square_19.tmp_0']} = square(inputs={X=['encoder_layer_1_ffn_fc_1.w_0@GRAD']}, op_device = , op_namescope = /gradient_clip_@CLIP_19/, op_role = 2, op_role_var = ['encoder_layer_1_ffn_fc_1.w_0', 'elementwise_mul_19.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_19.tmp_0']} = reduce_sum(inputs={X=['square_19.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip_@CLIP_19/, op_role = 2, op_role_var = ['encoder_layer_1_ffn_fc_1.w_0', 'elementwise_mul_19.tmp_0'], out_dtype = -1, reduce_all = True)
    {Out=['square_20.tmp_0']} = square(inputs={X=['encoder_layer_1_multi_head_att_key_fc.b_0@GRAD']}, op_device = , op_namescope = /gradient_clip_@CLIP_20/, op_role = 2, op_role_var = ['encoder_layer_1_multi_head_att_key_fc.b_0', 'elementwise_mul_20.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_20.tmp_0']} = reduce_sum(inputs={X=['square_20.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip_@CLIP_20/, op_role = 2, op_role_var = ['encoder_layer_1_multi_head_att_key_fc.b_0', 'elementwise_mul_20.tmp_0'], out_dtype = -1, reduce_all = True)
    {Out=['square_21.tmp_0']} = square(inputs={X=['encoder_layer_1_multi_head_att_key_fc.w_0@GRAD']}, op_device = , op_namescope = /gradient_clip_@CLIP_21/, op_role = 2, op_role_var = ['encoder_layer_1_multi_head_att_key_fc.w_0', 'elementwise_mul_21.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_21.tmp_0']} = reduce_sum(inputs={X=['square_21.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip_@CLIP_21/, op_role = 2, op_role_var = ['encoder_layer_1_multi_head_att_key_fc.w_0', 'elementwise_mul_21.tmp_0'], out_dtype = -1, reduce_all = True)
    {Out=['square_22.tmp_0']} = square(inputs={X=['encoder_layer_1_multi_head_att_output_fc.b_0@GRAD']}, op_device = , op_namescope = /gradient_clip_@CLIP_22/, op_role = 2, op_role_var = ['encoder_layer_1_multi_head_att_output_fc.b_0', 'elementwise_mul_22.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_22.tmp_0']} = reduce_sum(inputs={X=['square_22.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip_@CLIP_22/, op_role = 2, op_role_var = ['encoder_layer_1_multi_head_att_output_fc.b_0', 'elementwise_mul_22.tmp_0'], out_dtype = -1, reduce_all = True)
    {Out=['square_23.tmp_0']} = square(inputs={X=['encoder_layer_1_multi_head_att_output_fc.w_0@GRAD']}, op_device = , op_namescope = /gradient_clip_@CLIP_23/, op_role = 2, op_role_var = ['encoder_layer_1_multi_head_att_output_fc.w_0', 'elementwise_mul_23.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_23.tmp_0']} = reduce_sum(inputs={X=['square_23.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip_@CLIP_23/, op_role = 2, op_role_var = ['encoder_layer_1_multi_head_att_output_fc.w_0', 'elementwise_mul_23.tmp_0'], out_dtype = -1, reduce_all = True)
    {Out=['square_24.tmp_0']} = square(inputs={X=['encoder_layer_1_multi_head_att_query_fc.b_0@GRAD']}, op_device = , op_namescope = /gradient_clip_@CLIP_24/, op_role = 2, op_role_var = ['encoder_layer_1_multi_head_att_query_fc.b_0', 'elementwise_mul_24.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_24.tmp_0']} = reduce_sum(inputs={X=['square_24.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip_@CLIP_24/, op_role = 2, op_role_var = ['encoder_layer_1_multi_head_att_query_fc.b_0', 'elementwise_mul_24.tmp_0'], out_dtype = -1, reduce_all = True)
    {Out=['square_25.tmp_0']} = square(inputs={X=['encoder_layer_1_multi_head_att_query_fc.w_0@GRAD']}, op_device = , op_namescope = /gradient_clip_@CLIP_25/, op_role = 2, op_role_var = ['encoder_layer_1_multi_head_att_query_fc.w_0', 'elementwise_mul_25.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_25.tmp_0']} = reduce_sum(inputs={X=['square_25.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip_@CLIP_25/, op_role = 2, op_role_var = ['encoder_layer_1_multi_head_att_query_fc.w_0', 'elementwise_mul_25.tmp_0'], out_dtype = -1, reduce_all = True)
    {Out=['square_26.tmp_0']} = square(inputs={X=['encoder_layer_1_multi_head_att_value_fc.b_0@GRAD']}, op_device = , op_namescope = /gradient_clip_@CLIP_26/, op_role = 2, op_role_var = ['encoder_layer_1_multi_head_att_value_fc.b_0', 'elementwise_mul_26.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_26.tmp_0']} = reduce_sum(inputs={X=['square_26.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip_@CLIP_26/, op_role = 2, op_role_var = ['encoder_layer_1_multi_head_att_value_fc.b_0', 'elementwise_mul_26.tmp_0'], out_dtype = -1, reduce_all = True)
    {Out=['square_27.tmp_0']} = square(inputs={X=['encoder_layer_1_multi_head_att_value_fc.w_0@GRAD']}, op_device = , op_namescope = /gradient_clip_@CLIP_27/, op_role = 2, op_role_var = ['encoder_layer_1_multi_head_att_value_fc.w_0', 'elementwise_mul_27.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_27.tmp_0']} = reduce_sum(inputs={X=['square_27.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip_@CLIP_27/, op_role = 2, op_role_var = ['encoder_layer_1_multi_head_att_value_fc.w_0', 'elementwise_mul_27.tmp_0'], out_dtype = -1, reduce_all = True)
    {Out=['square_28.tmp_0']} = square(inputs={X=['encoder_layer_1_post_att_layer_norm_bias@GRAD']}, op_device = , op_namescope = /gradient_clip_@CLIP_28/, op_role = 2, op_role_var = ['encoder_layer_1_post_att_layer_norm_bias', 'elementwise_mul_28.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_28.tmp_0']} = reduce_sum(inputs={X=['square_28.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip_@CLIP_28/, op_role = 2, op_role_var = ['encoder_layer_1_post_att_layer_norm_bias', 'elementwise_mul_28.tmp_0'], out_dtype = -1, reduce_all = True)
    {Out=['square_29.tmp_0']} = square(inputs={X=['encoder_layer_1_post_att_layer_norm_scale@GRAD']}, op_device = , op_namescope = /gradient_clip_@CLIP_29/, op_role = 2, op_role_var = ['encoder_layer_1_post_att_layer_norm_scale', 'elementwise_mul_29.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_29.tmp_0']} = reduce_sum(inputs={X=['square_29.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip_@CLIP_29/, op_role = 2, op_role_var = ['encoder_layer_1_post_att_layer_norm_scale', 'elementwise_mul_29.tmp_0'], out_dtype = -1, reduce_all = True)
    {Out=['square_30.tmp_0']} = square(inputs={X=['encoder_layer_1_post_ffn_layer_norm_bias@GRAD']}, op_device = , op_namescope = /gradient_clip_@CLIP_30/, op_role = 2, op_role_var = ['encoder_layer_1_post_ffn_layer_norm_bias', 'elementwise_mul_30.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_30.tmp_0']} = reduce_sum(inputs={X=['square_30.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip_@CLIP_30/, op_role = 2, op_role_var = ['encoder_layer_1_post_ffn_layer_norm_bias', 'elementwise_mul_30.tmp_0'], out_dtype = -1, reduce_all = True)
    {Out=['square_31.tmp_0']} = square(inputs={X=['encoder_layer_1_post_ffn_layer_norm_scale@GRAD']}, op_device = , op_namescope = /gradient_clip_@CLIP_31/, op_role = 2, op_role_var = ['encoder_layer_1_post_ffn_layer_norm_scale', 'elementwise_mul_31.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_31.tmp_0']} = reduce_sum(inputs={X=['square_31.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip_@CLIP_31/, op_role = 2, op_role_var = ['encoder_layer_1_post_ffn_layer_norm_scale', 'elementwise_mul_31.tmp_0'], out_dtype = -1, reduce_all = True)
    {Out=['square_32.tmp_0']} = square(inputs={X=['encoder_layer_2_ffn_fc_0.b_0@GRAD']}, op_device = , op_namescope = /gradient_clip_@CLIP_32/, op_role = 2, op_role_var = ['encoder_layer_2_ffn_fc_0.b_0', 'elementwise_mul_32.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_32.tmp_0']} = reduce_sum(inputs={X=['square_32.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip_@CLIP_32/, op_role = 2, op_role_var = ['encoder_layer_2_ffn_fc_0.b_0', 'elementwise_mul_32.tmp_0'], out_dtype = -1, reduce_all = True)
    {Out=['square_33.tmp_0']} = square(inputs={X=['encoder_layer_2_ffn_fc_0.w_0@GRAD']}, op_device = , op_namescope = /gradient_clip_@CLIP_33/, op_role = 2, op_role_var = ['encoder_layer_2_ffn_fc_0.w_0', 'elementwise_mul_33.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_33.tmp_0']} = reduce_sum(inputs={X=['square_33.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip_@CLIP_33/, op_role = 2, op_role_var = ['encoder_layer_2_ffn_fc_0.w_0', 'elementwise_mul_33.tmp_0'], out_dtype = -1, reduce_all = True)
    {Out=['square_34.tmp_0']} = square(inputs={X=['encoder_layer_2_ffn_fc_1.b_0@GRAD']}, op_device = , op_namescope = /gradient_clip_@CLIP_34/, op_role = 2, op_role_var = ['encoder_layer_2_ffn_fc_1.b_0', 'elementwise_mul_34.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_34.tmp_0']} = reduce_sum(inputs={X=['square_34.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip_@CLIP_34/, op_role = 2, op_role_var = ['encoder_layer_2_ffn_fc_1.b_0', 'elementwise_mul_34.tmp_0'], out_dtype = -1, reduce_all = True)
    {Out=['square_35.tmp_0']} = square(inputs={X=['encoder_layer_2_ffn_fc_1.w_0@GRAD']}, op_device = , op_namescope = /gradient_clip_@CLIP_35/, op_role = 2, op_role_var = ['encoder_layer_2_ffn_fc_1.w_0', 'elementwise_mul_35.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_35.tmp_0']} = reduce_sum(inputs={X=['square_35.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip_@CLIP_35/, op_role = 2, op_role_var = ['encoder_layer_2_ffn_fc_1.w_0', 'elementwise_mul_35.tmp_0'], out_dtype = -1, reduce_all = True)
    {Out=['square_36.tmp_0']} = square(inputs={X=['encoder_layer_2_multi_head_att_key_fc.b_0@GRAD']}, op_device = , op_namescope = /gradient_clip_@CLIP_36/, op_role = 2, op_role_var = ['encoder_layer_2_multi_head_att_key_fc.b_0', 'elementwise_mul_36.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_36.tmp_0']} = reduce_sum(inputs={X=['square_36.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip_@CLIP_36/, op_role = 2, op_role_var = ['encoder_layer_2_multi_head_att_key_fc.b_0', 'elementwise_mul_36.tmp_0'], out_dtype = -1, reduce_all = True)
    {Out=['square_37.tmp_0']} = square(inputs={X=['encoder_layer_2_multi_head_att_key_fc.w_0@GRAD']}, op_device = , op_namescope = /gradient_clip_@CLIP_37/, op_role = 2, op_role_var = ['encoder_layer_2_multi_head_att_key_fc.w_0', 'elementwise_mul_37.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_37.tmp_0']} = reduce_sum(inputs={X=['square_37.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip_@CLIP_37/, op_role = 2, op_role_var = ['encoder_layer_2_multi_head_att_key_fc.w_0', 'elementwise_mul_37.tmp_0'], out_dtype = -1, reduce_all = True)
    {Out=['square_38.tmp_0']} = square(inputs={X=['encoder_layer_2_multi_head_att_output_fc.b_0@GRAD']}, op_device = , op_namescope = /gradient_clip_@CLIP_38/, op_role = 2, op_role_var = ['encoder_layer_2_multi_head_att_output_fc.b_0', 'elementwise_mul_38.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_38.tmp_0']} = reduce_sum(inputs={X=['square_38.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip_@CLIP_38/, op_role = 2, op_role_var = ['encoder_layer_2_multi_head_att_output_fc.b_0', 'elementwise_mul_38.tmp_0'], out_dtype = -1, reduce_all = True)
    {Out=['square_39.tmp_0']} = square(inputs={X=['encoder_layer_2_multi_head_att_output_fc.w_0@GRAD']}, op_device = , op_namescope = /gradient_clip_@CLIP_39/, op_role = 2, op_role_var = ['encoder_layer_2_multi_head_att_output_fc.w_0', 'elementwise_mul_39.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_39.tmp_0']} = reduce_sum(inputs={X=['square_39.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip_@CLIP_39/, op_role = 2, op_role_var = ['encoder_layer_2_multi_head_att_output_fc.w_0', 'elementwise_mul_39.tmp_0'], out_dtype = -1, reduce_all = True)
    {Out=['square_40.tmp_0']} = square(inputs={X=['encoder_layer_2_multi_head_att_query_fc.b_0@GRAD']}, op_device = , op_namescope = /gradient_clip_@CLIP_40/, op_role = 2, op_role_var = ['encoder_layer_2_multi_head_att_query_fc.b_0', 'elementwise_mul_40.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_40.tmp_0']} = reduce_sum(inputs={X=['square_40.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip_@CLIP_40/, op_role = 2, op_role_var = ['encoder_layer_2_multi_head_att_query_fc.b_0', 'elementwise_mul_40.tmp_0'], out_dtype = -1, reduce_all = True)
    {Out=['square_41.tmp_0']} = square(inputs={X=['encoder_layer_2_multi_head_att_query_fc.w_0@GRAD']}, op_device = , op_namescope = /gradient_clip_@CLIP_41/, op_role = 2, op_role_var = ['encoder_layer_2_multi_head_att_query_fc.w_0', 'elementwise_mul_41.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_41.tmp_0']} = reduce_sum(inputs={X=['square_41.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip_@CLIP_41/, op_role = 2, op_role_var = ['encoder_layer_2_multi_head_att_query_fc.w_0', 'elementwise_mul_41.tmp_0'], out_dtype = -1, reduce_all = True)
    {Out=['square_42.tmp_0']} = square(inputs={X=['encoder_layer_2_multi_head_att_value_fc.b_0@GRAD']}, op_device = , op_namescope = /gradient_clip_@CLIP_42/, op_role = 2, op_role_var = ['encoder_layer_2_multi_head_att_value_fc.b_0', 'elementwise_mul_42.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_42.tmp_0']} = reduce_sum(inputs={X=['square_42.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip_@CLIP_42/, op_role = 2, op_role_var = ['encoder_layer_2_multi_head_att_value_fc.b_0', 'elementwise_mul_42.tmp_0'], out_dtype = -1, reduce_all = True)
    {Out=['square_43.tmp_0']} = square(inputs={X=['encoder_layer_2_multi_head_att_value_fc.w_0@GRAD']}, op_device = , op_namescope = /gradient_clip_@CLIP_43/, op_role = 2, op_role_var = ['encoder_layer_2_multi_head_att_value_fc.w_0', 'elementwise_mul_43.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_43.tmp_0']} = reduce_sum(inputs={X=['square_43.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip_@CLIP_43/, op_role = 2, op_role_var = ['encoder_layer_2_multi_head_att_value_fc.w_0', 'elementwise_mul_43.tmp_0'], out_dtype = -1, reduce_all = True)
    {Out=['square_44.tmp_0']} = square(inputs={X=['encoder_layer_2_post_att_layer_norm_bias@GRAD']}, op_device = , op_namescope = /gradient_clip_@CLIP_44/, op_role = 2, op_role_var = ['encoder_layer_2_post_att_layer_norm_bias', 'elementwise_mul_44.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_44.tmp_0']} = reduce_sum(inputs={X=['square_44.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip_@CLIP_44/, op_role = 2, op_role_var = ['encoder_layer_2_post_att_layer_norm_bias', 'elementwise_mul_44.tmp_0'], out_dtype = -1, reduce_all = True)
    {Out=['square_45.tmp_0']} = square(inputs={X=['encoder_layer_2_post_att_layer_norm_scale@GRAD']}, op_device = , op_namescope = /gradient_clip_@CLIP_45/, op_role = 2, op_role_var = ['encoder_layer_2_post_att_layer_norm_scale', 'elementwise_mul_45.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_45.tmp_0']} = reduce_sum(inputs={X=['square_45.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip_@CLIP_45/, op_role = 2, op_role_var = ['encoder_layer_2_post_att_layer_norm_scale', 'elementwise_mul_45.tmp_0'], out_dtype = -1, reduce_all = True)
    {Out=['square_46.tmp_0']} = square(inputs={X=['encoder_layer_2_post_ffn_layer_norm_bias@GRAD']}, op_device = , op_namescope = /gradient_clip_@CLIP_46/, op_role = 2, op_role_var = ['encoder_layer_2_post_ffn_layer_norm_bias', 'elementwise_mul_46.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_46.tmp_0']} = reduce_sum(inputs={X=['square_46.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip_@CLIP_46/, op_role = 2, op_role_var = ['encoder_layer_2_post_ffn_layer_norm_bias', 'elementwise_mul_46.tmp_0'], out_dtype = -1, reduce_all = True)
    {Out=['square_47.tmp_0']} = square(inputs={X=['encoder_layer_2_post_ffn_layer_norm_scale@GRAD']}, op_device = , op_namescope = /gradient_clip_@CLIP_47/, op_role = 2, op_role_var = ['encoder_layer_2_post_ffn_layer_norm_scale', 'elementwise_mul_47.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_47.tmp_0']} = reduce_sum(inputs={X=['square_47.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip_@CLIP_47/, op_role = 2, op_role_var = ['encoder_layer_2_post_ffn_layer_norm_scale', 'elementwise_mul_47.tmp_0'], out_dtype = -1, reduce_all = True)
    {Out=['square_48.tmp_0']} = square(inputs={X=['mask_lm_out_fc.b_0@GRAD']}, op_device = , op_namescope = /gradient_clip_@CLIP_48/, op_role = 2, op_role_var = ['mask_lm_out_fc.b_0', 'elementwise_mul_48.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_48.tmp_0']} = reduce_sum(inputs={X=['square_48.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip_@CLIP_48/, op_role = 2, op_role_var = ['mask_lm_out_fc.b_0', 'elementwise_mul_48.tmp_0'], out_dtype = -1, reduce_all = True)
    {Out=['square_49.tmp_0']} = square(inputs={X=['mask_lm_trans_fc.b_0@GRAD']}, op_device = , op_namescope = /gradient_clip_@CLIP_49/, op_role = 2, op_role_var = ['mask_lm_trans_fc.b_0', 'elementwise_mul_49.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_49.tmp_0']} = reduce_sum(inputs={X=['square_49.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip_@CLIP_49/, op_role = 2, op_role_var = ['mask_lm_trans_fc.b_0', 'elementwise_mul_49.tmp_0'], out_dtype = -1, reduce_all = True)
    {Out=['square_50.tmp_0']} = square(inputs={X=['mask_lm_trans_fc.w_0@GRAD']}, op_device = , op_namescope = /gradient_clip_@CLIP_50/, op_role = 2, op_role_var = ['mask_lm_trans_fc.w_0', 'elementwise_mul_50.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_50.tmp_0']} = reduce_sum(inputs={X=['square_50.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip_@CLIP_50/, op_role = 2, op_role_var = ['mask_lm_trans_fc.w_0', 'elementwise_mul_50.tmp_0'], out_dtype = -1, reduce_all = True)
    {Out=['square_51.tmp_0']} = square(inputs={X=['mask_lm_trans_layer_norm_bias@GRAD']}, op_device = , op_namescope = /gradient_clip_@CLIP_51/, op_role = 2, op_role_var = ['mask_lm_trans_layer_norm_bias', 'elementwise_mul_51.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_51.tmp_0']} = reduce_sum(inputs={X=['square_51.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip_@CLIP_51/, op_role = 2, op_role_var = ['mask_lm_trans_layer_norm_bias', 'elementwise_mul_51.tmp_0'], out_dtype = -1, reduce_all = True)
    {Out=['square_52.tmp_0']} = square(inputs={X=['mask_lm_trans_layer_norm_scale@GRAD']}, op_device = , op_namescope = /gradient_clip_@CLIP_52/, op_role = 2, op_role_var = ['mask_lm_trans_layer_norm_scale', 'elementwise_mul_52.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_52.tmp_0']} = reduce_sum(inputs={X=['square_52.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip_@CLIP_52/, op_role = 2, op_role_var = ['mask_lm_trans_layer_norm_scale', 'elementwise_mul_52.tmp_0'], out_dtype = -1, reduce_all = True)
    {Out=['square_53.tmp_0']} = square(inputs={X=['pos_embedding@GRAD']}, op_device = , op_namescope = /gradient_clip_@CLIP_53/, op_role = 2, op_role_var = ['pos_embedding', 'elementwise_mul_53.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_53.tmp_0']} = reduce_sum(inputs={X=['square_53.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip_@CLIP_53/, op_role = 2, op_role_var = ['pos_embedding', 'elementwise_mul_53.tmp_0'], out_dtype = -1, reduce_all = True)
    {Out=['square_54.tmp_0']} = square(inputs={X=['pre_encoder_layer_norm_bias@GRAD']}, op_device = , op_namescope = /gradient_clip_@CLIP_54/, op_role = 2, op_role_var = ['pre_encoder_layer_norm_bias', 'elementwise_mul_54.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_54.tmp_0']} = reduce_sum(inputs={X=['square_54.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip_@CLIP_54/, op_role = 2, op_role_var = ['pre_encoder_layer_norm_bias', 'elementwise_mul_54.tmp_0'], out_dtype = -1, reduce_all = True)
    {Out=['square_55.tmp_0']} = square(inputs={X=['pre_encoder_layer_norm_scale@GRAD']}, op_device = , op_namescope = /gradient_clip_@CLIP_55/, op_role = 2, op_role_var = ['pre_encoder_layer_norm_scale', 'elementwise_mul_55.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_55.tmp_0']} = reduce_sum(inputs={X=['square_55.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip_@CLIP_55/, op_role = 2, op_role_var = ['pre_encoder_layer_norm_scale', 'elementwise_mul_55.tmp_0'], out_dtype = -1, reduce_all = True)
    {Out=['square_56.tmp_0']} = square(inputs={X=['sent_embedding@GRAD']}, op_device = , op_namescope = /gradient_clip_@CLIP_56/, op_role = 2, op_role_var = ['sent_embedding', 'elementwise_mul_56.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_56.tmp_0']} = reduce_sum(inputs={X=['square_56.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip_@CLIP_56/, op_role = 2, op_role_var = ['sent_embedding', 'elementwise_mul_56.tmp_0'], out_dtype = -1, reduce_all = True)
    {Out=['square_57.tmp_0']} = square(inputs={X=['task_embedding@GRAD']}, op_device = , op_namescope = /gradient_clip_@CLIP_57/, op_role = 2, op_role_var = ['task_embedding', 'elementwise_mul_57.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_57.tmp_0']} = reduce_sum(inputs={X=['square_57.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip_@CLIP_57/, op_role = 2, op_role_var = ['task_embedding', 'elementwise_mul_57.tmp_0'], out_dtype = -1, reduce_all = True)
    {Out=['square_58.tmp_0']} = square(inputs={X=['word_embedding@GRAD']}, op_device = , op_namescope = /gradient_clip_@CLIP_58/, op_role = 2, op_role_var = ['word_embedding', 'elementwise_mul_58.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_58.tmp_0']} = reduce_sum(inputs={X=['square_58.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip_@CLIP_58/, op_role = 2, op_role_var = ['word_embedding', 'elementwise_mul_58.tmp_0'], out_dtype = -1, reduce_all = True)
    {Out=['sum_0.tmp_0']} = sum(inputs={X=['reduce_sum_0.tmp_0', 'reduce_sum_1.tmp_0', 'reduce_sum_2.tmp_0', 'reduce_sum_3.tmp_0', 'reduce_sum_4.tmp_0', 'reduce_sum_5.tmp_0', 'reduce_sum_6.tmp_0', 'reduce_sum_7.tmp_0', 'reduce_sum_8.tmp_0', 'reduce_sum_9.tmp_0', 'reduce_sum_10.tmp_0', 'reduce_sum_11.tmp_0', 'reduce_sum_12.tmp_0', 'reduce_sum_13.tmp_0', 'reduce_sum_14.tmp_0', 'reduce_sum_15.tmp_0', 'reduce_sum_16.tmp_0', 'reduce_sum_17.tmp_0', 'reduce_sum_18.tmp_0', 'reduce_sum_19.tmp_0', 'reduce_sum_20.tmp_0', 'reduce_sum_21.tmp_0', 'reduce_sum_22.tmp_0', 'reduce_sum_23.tmp_0', 'reduce_sum_24.tmp_0', 'reduce_sum_25.tmp_0', 'reduce_sum_26.tmp_0', 'reduce_sum_27.tmp_0', 'reduce_sum_28.tmp_0', 'reduce_sum_29.tmp_0', 'reduce_sum_30.tmp_0', 'reduce_sum_31.tmp_0', 'reduce_sum_32.tmp_0', 'reduce_sum_33.tmp_0', 'reduce_sum_34.tmp_0', 'reduce_sum_35.tmp_0', 'reduce_sum_36.tmp_0', 'reduce_sum_37.tmp_0', 'reduce_sum_38.tmp_0', 'reduce_sum_39.tmp_0', 'reduce_sum_40.tmp_0', 'reduce_sum_41.tmp_0', 'reduce_sum_42.tmp_0', 'reduce_sum_43.tmp_0', 'reduce_sum_44.tmp_0', 'reduce_sum_45.tmp_0', 'reduce_sum_46.tmp_0', 'reduce_sum_47.tmp_0', 'reduce_sum_48.tmp_0', 'reduce_sum_49.tmp_0', 'reduce_sum_50.tmp_0', 'reduce_sum_51.tmp_0', 'reduce_sum_52.tmp_0', 'reduce_sum_53.tmp_0', 'reduce_sum_54.tmp_0', 'reduce_sum_55.tmp_0', 'reduce_sum_56.tmp_0', 'reduce_sum_57.tmp_0', 'reduce_sum_58.tmp_0']}, op_device = , op_namescope = /graident_clip_@CLIP/, op_role = 2, op_role_var = ['encoder_layer_0_ffn_fc_0.b_0', 'encoder_layer_0_ffn_fc_0.b_0@GRAD'], use_mkldnn = False)
    {Out=['sqrt_0.tmp_0']} = sqrt(inputs={X=['sum_0.tmp_0']}, op_device = , op_namescope = /graident_clip_@CLIP/, op_role = 2, op_role_var = ['encoder_layer_0_ffn_fc_0.b_0', 'encoder_layer_0_ffn_fc_0.b_0@GRAD'], use_cudnn = False, use_mkldnn = False)
    {Out=['elementwise_max_0.tmp_0']} = elementwise_max(inputs={X=['fill_constant_1.tmp_0'], Y=['sqrt_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /graident_clip_@CLIP/, op_role = 2, op_role_var = ['encoder_layer_0_ffn_fc_0.b_0', 'encoder_layer_0_ffn_fc_0.b_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_div_0.tmp_0']} = elementwise_div(inputs={X=['fill_constant_1.tmp_0'], Y=['elementwise_max_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /graident_clip_@CLIP/, op_role = 2, op_role_var = ['encoder_layer_0_ffn_fc_0.b_0', 'encoder_layer_0_ffn_fc_0.b_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_0.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_0_ffn_fc_0.b_0@GRAD'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /graident_clip_@CLIP/, op_role = 2, op_role_var = ['encoder_layer_0_ffn_fc_0.b_0', 'encoder_layer_0_ffn_fc_0.b_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_1.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_0_ffn_fc_0.w_0@GRAD'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /graident_clip_@CLIP_1/, op_role = 2, op_role_var = ['encoder_layer_0_ffn_fc_0.w_0', 'encoder_layer_0_ffn_fc_0.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_2.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_0_ffn_fc_1.b_0@GRAD'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /graident_clip_@CLIP_2/, op_role = 2, op_role_var = ['encoder_layer_0_ffn_fc_1.b_0', 'encoder_layer_0_ffn_fc_1.b_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_3.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_0_ffn_fc_1.w_0@GRAD'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /graident_clip_@CLIP_3/, op_role = 2, op_role_var = ['encoder_layer_0_ffn_fc_1.w_0', 'encoder_layer_0_ffn_fc_1.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_4.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_0_multi_head_att_key_fc.b_0@GRAD'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /graident_clip_@CLIP_4/, op_role = 2, op_role_var = ['encoder_layer_0_multi_head_att_key_fc.b_0', 'encoder_layer_0_multi_head_att_key_fc.b_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_5.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_0_multi_head_att_key_fc.w_0@GRAD'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /graident_clip_@CLIP_5/, op_role = 2, op_role_var = ['encoder_layer_0_multi_head_att_key_fc.w_0', 'encoder_layer_0_multi_head_att_key_fc.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_6.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_0_multi_head_att_output_fc.b_0@GRAD'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /graident_clip_@CLIP_6/, op_role = 2, op_role_var = ['encoder_layer_0_multi_head_att_output_fc.b_0', 'encoder_layer_0_multi_head_att_output_fc.b_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_7.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_0_multi_head_att_output_fc.w_0@GRAD'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /graident_clip_@CLIP_7/, op_role = 2, op_role_var = ['encoder_layer_0_multi_head_att_output_fc.w_0', 'encoder_layer_0_multi_head_att_output_fc.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_8.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_0_multi_head_att_query_fc.b_0@GRAD'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /graident_clip_@CLIP_8/, op_role = 2, op_role_var = ['encoder_layer_0_multi_head_att_query_fc.b_0', 'encoder_layer_0_multi_head_att_query_fc.b_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_9.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_0_multi_head_att_query_fc.w_0@GRAD'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /graident_clip_@CLIP_9/, op_role = 2, op_role_var = ['encoder_layer_0_multi_head_att_query_fc.w_0', 'encoder_layer_0_multi_head_att_query_fc.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_10.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_0_multi_head_att_value_fc.b_0@GRAD'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /graident_clip_@CLIP_10/, op_role = 2, op_role_var = ['encoder_layer_0_multi_head_att_value_fc.b_0', 'encoder_layer_0_multi_head_att_value_fc.b_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_11.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_0_multi_head_att_value_fc.w_0@GRAD'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /graident_clip_@CLIP_11/, op_role = 2, op_role_var = ['encoder_layer_0_multi_head_att_value_fc.w_0', 'encoder_layer_0_multi_head_att_value_fc.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_12.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_0_post_att_layer_norm_bias@GRAD'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /graident_clip_@CLIP_12/, op_role = 2, op_role_var = ['encoder_layer_0_post_att_layer_norm_bias', 'encoder_layer_0_post_att_layer_norm_bias@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_13.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_0_post_att_layer_norm_scale@GRAD'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /graident_clip_@CLIP_13/, op_role = 2, op_role_var = ['encoder_layer_0_post_att_layer_norm_scale', 'encoder_layer_0_post_att_layer_norm_scale@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_14.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_0_post_ffn_layer_norm_bias@GRAD'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /graident_clip_@CLIP_14/, op_role = 2, op_role_var = ['encoder_layer_0_post_ffn_layer_norm_bias', 'encoder_layer_0_post_ffn_layer_norm_bias@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_15.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_0_post_ffn_layer_norm_scale@GRAD'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /graident_clip_@CLIP_15/, op_role = 2, op_role_var = ['encoder_layer_0_post_ffn_layer_norm_scale', 'encoder_layer_0_post_ffn_layer_norm_scale@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_16.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_1_ffn_fc_0.b_0@GRAD'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /graident_clip_@CLIP_16/, op_role = 2, op_role_var = ['encoder_layer_1_ffn_fc_0.b_0', 'encoder_layer_1_ffn_fc_0.b_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_17.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_1_ffn_fc_0.w_0@GRAD'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /graident_clip_@CLIP_17/, op_role = 2, op_role_var = ['encoder_layer_1_ffn_fc_0.w_0', 'encoder_layer_1_ffn_fc_0.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_18.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_1_ffn_fc_1.b_0@GRAD'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /graident_clip_@CLIP_18/, op_role = 2, op_role_var = ['encoder_layer_1_ffn_fc_1.b_0', 'encoder_layer_1_ffn_fc_1.b_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_19.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_1_ffn_fc_1.w_0@GRAD'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /graident_clip_@CLIP_19/, op_role = 2, op_role_var = ['encoder_layer_1_ffn_fc_1.w_0', 'encoder_layer_1_ffn_fc_1.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_20.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_1_multi_head_att_key_fc.b_0@GRAD'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /graident_clip_@CLIP_20/, op_role = 2, op_role_var = ['encoder_layer_1_multi_head_att_key_fc.b_0', 'encoder_layer_1_multi_head_att_key_fc.b_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_21.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_1_multi_head_att_key_fc.w_0@GRAD'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /graident_clip_@CLIP_21/, op_role = 2, op_role_var = ['encoder_layer_1_multi_head_att_key_fc.w_0', 'encoder_layer_1_multi_head_att_key_fc.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_22.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_1_multi_head_att_output_fc.b_0@GRAD'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /graident_clip_@CLIP_22/, op_role = 2, op_role_var = ['encoder_layer_1_multi_head_att_output_fc.b_0', 'encoder_layer_1_multi_head_att_output_fc.b_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_23.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_1_multi_head_att_output_fc.w_0@GRAD'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /graident_clip_@CLIP_23/, op_role = 2, op_role_var = ['encoder_layer_1_multi_head_att_output_fc.w_0', 'encoder_layer_1_multi_head_att_output_fc.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_24.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_1_multi_head_att_query_fc.b_0@GRAD'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /graident_clip_@CLIP_24/, op_role = 2, op_role_var = ['encoder_layer_1_multi_head_att_query_fc.b_0', 'encoder_layer_1_multi_head_att_query_fc.b_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_25.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_1_multi_head_att_query_fc.w_0@GRAD'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /graident_clip_@CLIP_25/, op_role = 2, op_role_var = ['encoder_layer_1_multi_head_att_query_fc.w_0', 'encoder_layer_1_multi_head_att_query_fc.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_26.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_1_multi_head_att_value_fc.b_0@GRAD'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /graident_clip_@CLIP_26/, op_role = 2, op_role_var = ['encoder_layer_1_multi_head_att_value_fc.b_0', 'encoder_layer_1_multi_head_att_value_fc.b_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_27.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_1_multi_head_att_value_fc.w_0@GRAD'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /graident_clip_@CLIP_27/, op_role = 2, op_role_var = ['encoder_layer_1_multi_head_att_value_fc.w_0', 'encoder_layer_1_multi_head_att_value_fc.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_28.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_1_post_att_layer_norm_bias@GRAD'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /graident_clip_@CLIP_28/, op_role = 2, op_role_var = ['encoder_layer_1_post_att_layer_norm_bias', 'encoder_layer_1_post_att_layer_norm_bias@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_29.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_1_post_att_layer_norm_scale@GRAD'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /graident_clip_@CLIP_29/, op_role = 2, op_role_var = ['encoder_layer_1_post_att_layer_norm_scale', 'encoder_layer_1_post_att_layer_norm_scale@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_30.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_1_post_ffn_layer_norm_bias@GRAD'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /graident_clip_@CLIP_30/, op_role = 2, op_role_var = ['encoder_layer_1_post_ffn_layer_norm_bias', 'encoder_layer_1_post_ffn_layer_norm_bias@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_31.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_1_post_ffn_layer_norm_scale@GRAD'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /graident_clip_@CLIP_31/, op_role = 2, op_role_var = ['encoder_layer_1_post_ffn_layer_norm_scale', 'encoder_layer_1_post_ffn_layer_norm_scale@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_32.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_2_ffn_fc_0.b_0@GRAD'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /graident_clip_@CLIP_32/, op_role = 2, op_role_var = ['encoder_layer_2_ffn_fc_0.b_0', 'encoder_layer_2_ffn_fc_0.b_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_33.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_2_ffn_fc_0.w_0@GRAD'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /graident_clip_@CLIP_33/, op_role = 2, op_role_var = ['encoder_layer_2_ffn_fc_0.w_0', 'encoder_layer_2_ffn_fc_0.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_34.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_2_ffn_fc_1.b_0@GRAD'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /graident_clip_@CLIP_34/, op_role = 2, op_role_var = ['encoder_layer_2_ffn_fc_1.b_0', 'encoder_layer_2_ffn_fc_1.b_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_35.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_2_ffn_fc_1.w_0@GRAD'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /graident_clip_@CLIP_35/, op_role = 2, op_role_var = ['encoder_layer_2_ffn_fc_1.w_0', 'encoder_layer_2_ffn_fc_1.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_36.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_2_multi_head_att_key_fc.b_0@GRAD'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /graident_clip_@CLIP_36/, op_role = 2, op_role_var = ['encoder_layer_2_multi_head_att_key_fc.b_0', 'encoder_layer_2_multi_head_att_key_fc.b_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_37.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_2_multi_head_att_key_fc.w_0@GRAD'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /graident_clip_@CLIP_37/, op_role = 2, op_role_var = ['encoder_layer_2_multi_head_att_key_fc.w_0', 'encoder_layer_2_multi_head_att_key_fc.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_38.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_2_multi_head_att_output_fc.b_0@GRAD'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /graident_clip_@CLIP_38/, op_role = 2, op_role_var = ['encoder_layer_2_multi_head_att_output_fc.b_0', 'encoder_layer_2_multi_head_att_output_fc.b_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_39.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_2_multi_head_att_output_fc.w_0@GRAD'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /graident_clip_@CLIP_39/, op_role = 2, op_role_var = ['encoder_layer_2_multi_head_att_output_fc.w_0', 'encoder_layer_2_multi_head_att_output_fc.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_40.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_2_multi_head_att_query_fc.b_0@GRAD'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /graident_clip_@CLIP_40/, op_role = 2, op_role_var = ['encoder_layer_2_multi_head_att_query_fc.b_0', 'encoder_layer_2_multi_head_att_query_fc.b_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_41.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_2_multi_head_att_query_fc.w_0@GRAD'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /graident_clip_@CLIP_41/, op_role = 2, op_role_var = ['encoder_layer_2_multi_head_att_query_fc.w_0', 'encoder_layer_2_multi_head_att_query_fc.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_42.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_2_multi_head_att_value_fc.b_0@GRAD'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /graident_clip_@CLIP_42/, op_role = 2, op_role_var = ['encoder_layer_2_multi_head_att_value_fc.b_0', 'encoder_layer_2_multi_head_att_value_fc.b_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_43.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_2_multi_head_att_value_fc.w_0@GRAD'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /graident_clip_@CLIP_43/, op_role = 2, op_role_var = ['encoder_layer_2_multi_head_att_value_fc.w_0', 'encoder_layer_2_multi_head_att_value_fc.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_44.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_2_post_att_layer_norm_bias@GRAD'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /graident_clip_@CLIP_44/, op_role = 2, op_role_var = ['encoder_layer_2_post_att_layer_norm_bias', 'encoder_layer_2_post_att_layer_norm_bias@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_45.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_2_post_att_layer_norm_scale@GRAD'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /graident_clip_@CLIP_45/, op_role = 2, op_role_var = ['encoder_layer_2_post_att_layer_norm_scale', 'encoder_layer_2_post_att_layer_norm_scale@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_46.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_2_post_ffn_layer_norm_bias@GRAD'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /graident_clip_@CLIP_46/, op_role = 2, op_role_var = ['encoder_layer_2_post_ffn_layer_norm_bias', 'encoder_layer_2_post_ffn_layer_norm_bias@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_47.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_2_post_ffn_layer_norm_scale@GRAD'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /graident_clip_@CLIP_47/, op_role = 2, op_role_var = ['encoder_layer_2_post_ffn_layer_norm_scale', 'encoder_layer_2_post_ffn_layer_norm_scale@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_48.tmp_0']} = elementwise_mul(inputs={X=['mask_lm_out_fc.b_0@GRAD'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /graident_clip_@CLIP_48/, op_role = 2, op_role_var = ['mask_lm_out_fc.b_0', 'mask_lm_out_fc.b_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_49.tmp_0']} = elementwise_mul(inputs={X=['mask_lm_trans_fc.b_0@GRAD'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /graident_clip_@CLIP_49/, op_role = 2, op_role_var = ['mask_lm_trans_fc.b_0', 'mask_lm_trans_fc.b_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_50.tmp_0']} = elementwise_mul(inputs={X=['mask_lm_trans_fc.w_0@GRAD'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /graident_clip_@CLIP_50/, op_role = 2, op_role_var = ['mask_lm_trans_fc.w_0', 'mask_lm_trans_fc.w_0@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_51.tmp_0']} = elementwise_mul(inputs={X=['mask_lm_trans_layer_norm_bias@GRAD'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /graident_clip_@CLIP_51/, op_role = 2, op_role_var = ['mask_lm_trans_layer_norm_bias', 'mask_lm_trans_layer_norm_bias@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_52.tmp_0']} = elementwise_mul(inputs={X=['mask_lm_trans_layer_norm_scale@GRAD'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /graident_clip_@CLIP_52/, op_role = 2, op_role_var = ['mask_lm_trans_layer_norm_scale', 'mask_lm_trans_layer_norm_scale@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_53.tmp_0']} = elementwise_mul(inputs={X=['pos_embedding@GRAD'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /graident_clip_@CLIP_53/, op_role = 2, op_role_var = ['pos_embedding', 'pos_embedding@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_54.tmp_0']} = elementwise_mul(inputs={X=['pre_encoder_layer_norm_bias@GRAD'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /graident_clip_@CLIP_54/, op_role = 2, op_role_var = ['pre_encoder_layer_norm_bias', 'pre_encoder_layer_norm_bias@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_55.tmp_0']} = elementwise_mul(inputs={X=['pre_encoder_layer_norm_scale@GRAD'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /graident_clip_@CLIP_55/, op_role = 2, op_role_var = ['pre_encoder_layer_norm_scale', 'pre_encoder_layer_norm_scale@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_56.tmp_0']} = elementwise_mul(inputs={X=['sent_embedding@GRAD'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /graident_clip_@CLIP_56/, op_role = 2, op_role_var = ['sent_embedding', 'sent_embedding@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_57.tmp_0']} = elementwise_mul(inputs={X=['task_embedding@GRAD'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /graident_clip_@CLIP_57/, op_role = 2, op_role_var = ['task_embedding', 'task_embedding@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_58.tmp_0']} = elementwise_mul(inputs={X=['word_embedding@GRAD'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /graident_clip_@CLIP_58/, op_role = 2, op_role_var = ['word_embedding', 'word_embedding@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {ParamOut=['encoder_layer_0_ffn_fc_0.b_0']} = sgd(inputs={Grad=['elementwise_mul_0.tmp_0'], LearningRate=['learning_rate_0'], Param=['encoder_layer_0_ffn_fc_0.b_0']}, op_device = , op_namescope = /optimizer/, op_role = 2, op_role_var = ['encoder_layer_0_ffn_fc_0.b_0', 'elementwise_mul_0.tmp_0'])
    {ParamOut=['encoder_layer_0_ffn_fc_0.w_0']} = sgd(inputs={Grad=['elementwise_mul_1.tmp_0'], LearningRate=['learning_rate_0'], Param=['encoder_layer_0_ffn_fc_0.w_0']}, op_device = , op_namescope = /optimizer_1/, op_role = 2, op_role_var = ['encoder_layer_0_ffn_fc_0.w_0', 'elementwise_mul_1.tmp_0'])
    {ParamOut=['encoder_layer_0_ffn_fc_1.b_0']} = sgd(inputs={Grad=['elementwise_mul_2.tmp_0'], LearningRate=['learning_rate_0'], Param=['encoder_layer_0_ffn_fc_1.b_0']}, op_device = , op_namescope = /optimizer_2/, op_role = 2, op_role_var = ['encoder_layer_0_ffn_fc_1.b_0', 'elementwise_mul_2.tmp_0'])
    {ParamOut=['encoder_layer_0_ffn_fc_1.w_0']} = sgd(inputs={Grad=['elementwise_mul_3.tmp_0'], LearningRate=['learning_rate_0'], Param=['encoder_layer_0_ffn_fc_1.w_0']}, op_device = , op_namescope = /optimizer_3/, op_role = 2, op_role_var = ['encoder_layer_0_ffn_fc_1.w_0', 'elementwise_mul_3.tmp_0'])
    {ParamOut=['encoder_layer_0_multi_head_att_key_fc.b_0']} = sgd(inputs={Grad=['elementwise_mul_4.tmp_0'], LearningRate=['learning_rate_0'], Param=['encoder_layer_0_multi_head_att_key_fc.b_0']}, op_device = , op_namescope = /optimizer_4/, op_role = 2, op_role_var = ['encoder_layer_0_multi_head_att_key_fc.b_0', 'elementwise_mul_4.tmp_0'])
    {ParamOut=['encoder_layer_0_multi_head_att_key_fc.w_0']} = sgd(inputs={Grad=['elementwise_mul_5.tmp_0'], LearningRate=['learning_rate_0'], Param=['encoder_layer_0_multi_head_att_key_fc.w_0']}, op_device = , op_namescope = /optimizer_5/, op_role = 2, op_role_var = ['encoder_layer_0_multi_head_att_key_fc.w_0', 'elementwise_mul_5.tmp_0'])
    {ParamOut=['encoder_layer_0_multi_head_att_output_fc.b_0']} = sgd(inputs={Grad=['elementwise_mul_6.tmp_0'], LearningRate=['learning_rate_0'], Param=['encoder_layer_0_multi_head_att_output_fc.b_0']}, op_device = , op_namescope = /optimizer_6/, op_role = 2, op_role_var = ['encoder_layer_0_multi_head_att_output_fc.b_0', 'elementwise_mul_6.tmp_0'])
    {ParamOut=['encoder_layer_0_multi_head_att_output_fc.w_0']} = sgd(inputs={Grad=['elementwise_mul_7.tmp_0'], LearningRate=['learning_rate_0'], Param=['encoder_layer_0_multi_head_att_output_fc.w_0']}, op_device = , op_namescope = /optimizer_7/, op_role = 2, op_role_var = ['encoder_layer_0_multi_head_att_output_fc.w_0', 'elementwise_mul_7.tmp_0'])
    {ParamOut=['encoder_layer_0_multi_head_att_query_fc.b_0']} = sgd(inputs={Grad=['elementwise_mul_8.tmp_0'], LearningRate=['learning_rate_0'], Param=['encoder_layer_0_multi_head_att_query_fc.b_0']}, op_device = , op_namescope = /optimizer_8/, op_role = 2, op_role_var = ['encoder_layer_0_multi_head_att_query_fc.b_0', 'elementwise_mul_8.tmp_0'])
    {ParamOut=['encoder_layer_0_multi_head_att_query_fc.w_0']} = sgd(inputs={Grad=['elementwise_mul_9.tmp_0'], LearningRate=['learning_rate_0'], Param=['encoder_layer_0_multi_head_att_query_fc.w_0']}, op_device = , op_namescope = /optimizer_9/, op_role = 2, op_role_var = ['encoder_layer_0_multi_head_att_query_fc.w_0', 'elementwise_mul_9.tmp_0'])
    {ParamOut=['encoder_layer_0_multi_head_att_value_fc.b_0']} = sgd(inputs={Grad=['elementwise_mul_10.tmp_0'], LearningRate=['learning_rate_0'], Param=['encoder_layer_0_multi_head_att_value_fc.b_0']}, op_device = , op_namescope = /optimizer_10/, op_role = 2, op_role_var = ['encoder_layer_0_multi_head_att_value_fc.b_0', 'elementwise_mul_10.tmp_0'])
    {ParamOut=['encoder_layer_0_multi_head_att_value_fc.w_0']} = sgd(inputs={Grad=['elementwise_mul_11.tmp_0'], LearningRate=['learning_rate_0'], Param=['encoder_layer_0_multi_head_att_value_fc.w_0']}, op_device = , op_namescope = /optimizer_11/, op_role = 2, op_role_var = ['encoder_layer_0_multi_head_att_value_fc.w_0', 'elementwise_mul_11.tmp_0'])
    {ParamOut=['encoder_layer_0_post_att_layer_norm_bias']} = sgd(inputs={Grad=['elementwise_mul_12.tmp_0'], LearningRate=['learning_rate_0'], Param=['encoder_layer_0_post_att_layer_norm_bias']}, op_device = , op_namescope = /optimizer_12/, op_role = 2, op_role_var = ['encoder_layer_0_post_att_layer_norm_bias', 'elementwise_mul_12.tmp_0'])
    {ParamOut=['encoder_layer_0_post_att_layer_norm_scale']} = sgd(inputs={Grad=['elementwise_mul_13.tmp_0'], LearningRate=['learning_rate_0'], Param=['encoder_layer_0_post_att_layer_norm_scale']}, op_device = , op_namescope = /optimizer_13/, op_role = 2, op_role_var = ['encoder_layer_0_post_att_layer_norm_scale', 'elementwise_mul_13.tmp_0'])
    {ParamOut=['encoder_layer_0_post_ffn_layer_norm_bias']} = sgd(inputs={Grad=['elementwise_mul_14.tmp_0'], LearningRate=['learning_rate_0'], Param=['encoder_layer_0_post_ffn_layer_norm_bias']}, op_device = , op_namescope = /optimizer_14/, op_role = 2, op_role_var = ['encoder_layer_0_post_ffn_layer_norm_bias', 'elementwise_mul_14.tmp_0'])
    {ParamOut=['encoder_layer_0_post_ffn_layer_norm_scale']} = sgd(inputs={Grad=['elementwise_mul_15.tmp_0'], LearningRate=['learning_rate_0'], Param=['encoder_layer_0_post_ffn_layer_norm_scale']}, op_device = , op_namescope = /optimizer_15/, op_role = 2, op_role_var = ['encoder_layer_0_post_ffn_layer_norm_scale', 'elementwise_mul_15.tmp_0'])
    {ParamOut=['encoder_layer_1_ffn_fc_0.b_0']} = sgd(inputs={Grad=['elementwise_mul_16.tmp_0'], LearningRate=['learning_rate_0'], Param=['encoder_layer_1_ffn_fc_0.b_0']}, op_device = , op_namescope = /optimizer_16/, op_role = 2, op_role_var = ['encoder_layer_1_ffn_fc_0.b_0', 'elementwise_mul_16.tmp_0'])
    {ParamOut=['encoder_layer_1_ffn_fc_0.w_0']} = sgd(inputs={Grad=['elementwise_mul_17.tmp_0'], LearningRate=['learning_rate_0'], Param=['encoder_layer_1_ffn_fc_0.w_0']}, op_device = , op_namescope = /optimizer_17/, op_role = 2, op_role_var = ['encoder_layer_1_ffn_fc_0.w_0', 'elementwise_mul_17.tmp_0'])
    {ParamOut=['encoder_layer_1_ffn_fc_1.b_0']} = sgd(inputs={Grad=['elementwise_mul_18.tmp_0'], LearningRate=['learning_rate_0'], Param=['encoder_layer_1_ffn_fc_1.b_0']}, op_device = , op_namescope = /optimizer_18/, op_role = 2, op_role_var = ['encoder_layer_1_ffn_fc_1.b_0', 'elementwise_mul_18.tmp_0'])
    {ParamOut=['encoder_layer_1_ffn_fc_1.w_0']} = sgd(inputs={Grad=['elementwise_mul_19.tmp_0'], LearningRate=['learning_rate_0'], Param=['encoder_layer_1_ffn_fc_1.w_0']}, op_device = , op_namescope = /optimizer_19/, op_role = 2, op_role_var = ['encoder_layer_1_ffn_fc_1.w_0', 'elementwise_mul_19.tmp_0'])
    {ParamOut=['encoder_layer_1_multi_head_att_key_fc.b_0']} = sgd(inputs={Grad=['elementwise_mul_20.tmp_0'], LearningRate=['learning_rate_0'], Param=['encoder_layer_1_multi_head_att_key_fc.b_0']}, op_device = , op_namescope = /optimizer_20/, op_role = 2, op_role_var = ['encoder_layer_1_multi_head_att_key_fc.b_0', 'elementwise_mul_20.tmp_0'])
    {ParamOut=['encoder_layer_1_multi_head_att_key_fc.w_0']} = sgd(inputs={Grad=['elementwise_mul_21.tmp_0'], LearningRate=['learning_rate_0'], Param=['encoder_layer_1_multi_head_att_key_fc.w_0']}, op_device = , op_namescope = /optimizer_21/, op_role = 2, op_role_var = ['encoder_layer_1_multi_head_att_key_fc.w_0', 'elementwise_mul_21.tmp_0'])
    {ParamOut=['encoder_layer_1_multi_head_att_output_fc.b_0']} = sgd(inputs={Grad=['elementwise_mul_22.tmp_0'], LearningRate=['learning_rate_0'], Param=['encoder_layer_1_multi_head_att_output_fc.b_0']}, op_device = , op_namescope = /optimizer_22/, op_role = 2, op_role_var = ['encoder_layer_1_multi_head_att_output_fc.b_0', 'elementwise_mul_22.tmp_0'])
    {ParamOut=['encoder_layer_1_multi_head_att_output_fc.w_0']} = sgd(inputs={Grad=['elementwise_mul_23.tmp_0'], LearningRate=['learning_rate_0'], Param=['encoder_layer_1_multi_head_att_output_fc.w_0']}, op_device = , op_namescope = /optimizer_23/, op_role = 2, op_role_var = ['encoder_layer_1_multi_head_att_output_fc.w_0', 'elementwise_mul_23.tmp_0'])
    {ParamOut=['encoder_layer_1_multi_head_att_query_fc.b_0']} = sgd(inputs={Grad=['elementwise_mul_24.tmp_0'], LearningRate=['learning_rate_0'], Param=['encoder_layer_1_multi_head_att_query_fc.b_0']}, op_device = , op_namescope = /optimizer_24/, op_role = 2, op_role_var = ['encoder_layer_1_multi_head_att_query_fc.b_0', 'elementwise_mul_24.tmp_0'])
    {ParamOut=['encoder_layer_1_multi_head_att_query_fc.w_0']} = sgd(inputs={Grad=['elementwise_mul_25.tmp_0'], LearningRate=['learning_rate_0'], Param=['encoder_layer_1_multi_head_att_query_fc.w_0']}, op_device = , op_namescope = /optimizer_25/, op_role = 2, op_role_var = ['encoder_layer_1_multi_head_att_query_fc.w_0', 'elementwise_mul_25.tmp_0'])
    {ParamOut=['encoder_layer_1_multi_head_att_value_fc.b_0']} = sgd(inputs={Grad=['elementwise_mul_26.tmp_0'], LearningRate=['learning_rate_0'], Param=['encoder_layer_1_multi_head_att_value_fc.b_0']}, op_device = , op_namescope = /optimizer_26/, op_role = 2, op_role_var = ['encoder_layer_1_multi_head_att_value_fc.b_0', 'elementwise_mul_26.tmp_0'])
    {ParamOut=['encoder_layer_1_multi_head_att_value_fc.w_0']} = sgd(inputs={Grad=['elementwise_mul_27.tmp_0'], LearningRate=['learning_rate_0'], Param=['encoder_layer_1_multi_head_att_value_fc.w_0']}, op_device = , op_namescope = /optimizer_27/, op_role = 2, op_role_var = ['encoder_layer_1_multi_head_att_value_fc.w_0', 'elementwise_mul_27.tmp_0'])
    {ParamOut=['encoder_layer_1_post_att_layer_norm_bias']} = sgd(inputs={Grad=['elementwise_mul_28.tmp_0'], LearningRate=['learning_rate_0'], Param=['encoder_layer_1_post_att_layer_norm_bias']}, op_device = , op_namescope = /optimizer_28/, op_role = 2, op_role_var = ['encoder_layer_1_post_att_layer_norm_bias', 'elementwise_mul_28.tmp_0'])
    {ParamOut=['encoder_layer_1_post_att_layer_norm_scale']} = sgd(inputs={Grad=['elementwise_mul_29.tmp_0'], LearningRate=['learning_rate_0'], Param=['encoder_layer_1_post_att_layer_norm_scale']}, op_device = , op_namescope = /optimizer_29/, op_role = 2, op_role_var = ['encoder_layer_1_post_att_layer_norm_scale', 'elementwise_mul_29.tmp_0'])
    {ParamOut=['encoder_layer_1_post_ffn_layer_norm_bias']} = sgd(inputs={Grad=['elementwise_mul_30.tmp_0'], LearningRate=['learning_rate_0'], Param=['encoder_layer_1_post_ffn_layer_norm_bias']}, op_device = , op_namescope = /optimizer_30/, op_role = 2, op_role_var = ['encoder_layer_1_post_ffn_layer_norm_bias', 'elementwise_mul_30.tmp_0'])
    {ParamOut=['encoder_layer_1_post_ffn_layer_norm_scale']} = sgd(inputs={Grad=['elementwise_mul_31.tmp_0'], LearningRate=['learning_rate_0'], Param=['encoder_layer_1_post_ffn_layer_norm_scale']}, op_device = , op_namescope = /optimizer_31/, op_role = 2, op_role_var = ['encoder_layer_1_post_ffn_layer_norm_scale', 'elementwise_mul_31.tmp_0'])
    {ParamOut=['encoder_layer_2_ffn_fc_0.b_0']} = sgd(inputs={Grad=['elementwise_mul_32.tmp_0'], LearningRate=['learning_rate_0'], Param=['encoder_layer_2_ffn_fc_0.b_0']}, op_device = , op_namescope = /optimizer_32/, op_role = 2, op_role_var = ['encoder_layer_2_ffn_fc_0.b_0', 'elementwise_mul_32.tmp_0'])
    {ParamOut=['encoder_layer_2_ffn_fc_0.w_0']} = sgd(inputs={Grad=['elementwise_mul_33.tmp_0'], LearningRate=['learning_rate_0'], Param=['encoder_layer_2_ffn_fc_0.w_0']}, op_device = , op_namescope = /optimizer_33/, op_role = 2, op_role_var = ['encoder_layer_2_ffn_fc_0.w_0', 'elementwise_mul_33.tmp_0'])
    {ParamOut=['encoder_layer_2_ffn_fc_1.b_0']} = sgd(inputs={Grad=['elementwise_mul_34.tmp_0'], LearningRate=['learning_rate_0'], Param=['encoder_layer_2_ffn_fc_1.b_0']}, op_device = , op_namescope = /optimizer_34/, op_role = 2, op_role_var = ['encoder_layer_2_ffn_fc_1.b_0', 'elementwise_mul_34.tmp_0'])
    {ParamOut=['encoder_layer_2_ffn_fc_1.w_0']} = sgd(inputs={Grad=['elementwise_mul_35.tmp_0'], LearningRate=['learning_rate_0'], Param=['encoder_layer_2_ffn_fc_1.w_0']}, op_device = , op_namescope = /optimizer_35/, op_role = 2, op_role_var = ['encoder_layer_2_ffn_fc_1.w_0', 'elementwise_mul_35.tmp_0'])
    {ParamOut=['encoder_layer_2_multi_head_att_key_fc.b_0']} = sgd(inputs={Grad=['elementwise_mul_36.tmp_0'], LearningRate=['learning_rate_0'], Param=['encoder_layer_2_multi_head_att_key_fc.b_0']}, op_device = , op_namescope = /optimizer_36/, op_role = 2, op_role_var = ['encoder_layer_2_multi_head_att_key_fc.b_0', 'elementwise_mul_36.tmp_0'])
    {ParamOut=['encoder_layer_2_multi_head_att_key_fc.w_0']} = sgd(inputs={Grad=['elementwise_mul_37.tmp_0'], LearningRate=['learning_rate_0'], Param=['encoder_layer_2_multi_head_att_key_fc.w_0']}, op_device = , op_namescope = /optimizer_37/, op_role = 2, op_role_var = ['encoder_layer_2_multi_head_att_key_fc.w_0', 'elementwise_mul_37.tmp_0'])
    {ParamOut=['encoder_layer_2_multi_head_att_output_fc.b_0']} = sgd(inputs={Grad=['elementwise_mul_38.tmp_0'], LearningRate=['learning_rate_0'], Param=['encoder_layer_2_multi_head_att_output_fc.b_0']}, op_device = , op_namescope = /optimizer_38/, op_role = 2, op_role_var = ['encoder_layer_2_multi_head_att_output_fc.b_0', 'elementwise_mul_38.tmp_0'])
    {ParamOut=['encoder_layer_2_multi_head_att_output_fc.w_0']} = sgd(inputs={Grad=['elementwise_mul_39.tmp_0'], LearningRate=['learning_rate_0'], Param=['encoder_layer_2_multi_head_att_output_fc.w_0']}, op_device = , op_namescope = /optimizer_39/, op_role = 2, op_role_var = ['encoder_layer_2_multi_head_att_output_fc.w_0', 'elementwise_mul_39.tmp_0'])
    {ParamOut=['encoder_layer_2_multi_head_att_query_fc.b_0']} = sgd(inputs={Grad=['elementwise_mul_40.tmp_0'], LearningRate=['learning_rate_0'], Param=['encoder_layer_2_multi_head_att_query_fc.b_0']}, op_device = , op_namescope = /optimizer_40/, op_role = 2, op_role_var = ['encoder_layer_2_multi_head_att_query_fc.b_0', 'elementwise_mul_40.tmp_0'])
    {ParamOut=['encoder_layer_2_multi_head_att_query_fc.w_0']} = sgd(inputs={Grad=['elementwise_mul_41.tmp_0'], LearningRate=['learning_rate_0'], Param=['encoder_layer_2_multi_head_att_query_fc.w_0']}, op_device = , op_namescope = /optimizer_41/, op_role = 2, op_role_var = ['encoder_layer_2_multi_head_att_query_fc.w_0', 'elementwise_mul_41.tmp_0'])
    {ParamOut=['encoder_layer_2_multi_head_att_value_fc.b_0']} = sgd(inputs={Grad=['elementwise_mul_42.tmp_0'], LearningRate=['learning_rate_0'], Param=['encoder_layer_2_multi_head_att_value_fc.b_0']}, op_device = , op_namescope = /optimizer_42/, op_role = 2, op_role_var = ['encoder_layer_2_multi_head_att_value_fc.b_0', 'elementwise_mul_42.tmp_0'])
    {ParamOut=['encoder_layer_2_multi_head_att_value_fc.w_0']} = sgd(inputs={Grad=['elementwise_mul_43.tmp_0'], LearningRate=['learning_rate_0'], Param=['encoder_layer_2_multi_head_att_value_fc.w_0']}, op_device = , op_namescope = /optimizer_43/, op_role = 2, op_role_var = ['encoder_layer_2_multi_head_att_value_fc.w_0', 'elementwise_mul_43.tmp_0'])
    {ParamOut=['encoder_layer_2_post_att_layer_norm_bias']} = sgd(inputs={Grad=['elementwise_mul_44.tmp_0'], LearningRate=['learning_rate_0'], Param=['encoder_layer_2_post_att_layer_norm_bias']}, op_device = , op_namescope = /optimizer_44/, op_role = 2, op_role_var = ['encoder_layer_2_post_att_layer_norm_bias', 'elementwise_mul_44.tmp_0'])
    {ParamOut=['encoder_layer_2_post_att_layer_norm_scale']} = sgd(inputs={Grad=['elementwise_mul_45.tmp_0'], LearningRate=['learning_rate_0'], Param=['encoder_layer_2_post_att_layer_norm_scale']}, op_device = , op_namescope = /optimizer_45/, op_role = 2, op_role_var = ['encoder_layer_2_post_att_layer_norm_scale', 'elementwise_mul_45.tmp_0'])
    {ParamOut=['encoder_layer_2_post_ffn_layer_norm_bias']} = sgd(inputs={Grad=['elementwise_mul_46.tmp_0'], LearningRate=['learning_rate_0'], Param=['encoder_layer_2_post_ffn_layer_norm_bias']}, op_device = , op_namescope = /optimizer_46/, op_role = 2, op_role_var = ['encoder_layer_2_post_ffn_layer_norm_bias', 'elementwise_mul_46.tmp_0'])
    {ParamOut=['encoder_layer_2_post_ffn_layer_norm_scale']} = sgd(inputs={Grad=['elementwise_mul_47.tmp_0'], LearningRate=['learning_rate_0'], Param=['encoder_layer_2_post_ffn_layer_norm_scale']}, op_device = , op_namescope = /optimizer_47/, op_role = 2, op_role_var = ['encoder_layer_2_post_ffn_layer_norm_scale', 'elementwise_mul_47.tmp_0'])
    {ParamOut=['mask_lm_out_fc.b_0']} = sgd(inputs={Grad=['elementwise_mul_48.tmp_0'], LearningRate=['learning_rate_0'], Param=['mask_lm_out_fc.b_0']}, op_device = , op_namescope = /optimizer_48/, op_role = 2, op_role_var = ['mask_lm_out_fc.b_0', 'elementwise_mul_48.tmp_0'])
    {ParamOut=['mask_lm_trans_fc.b_0']} = sgd(inputs={Grad=['elementwise_mul_49.tmp_0'], LearningRate=['learning_rate_0'], Param=['mask_lm_trans_fc.b_0']}, op_device = , op_namescope = /optimizer_49/, op_role = 2, op_role_var = ['mask_lm_trans_fc.b_0', 'elementwise_mul_49.tmp_0'])
    {ParamOut=['mask_lm_trans_fc.w_0']} = sgd(inputs={Grad=['elementwise_mul_50.tmp_0'], LearningRate=['learning_rate_0'], Param=['mask_lm_trans_fc.w_0']}, op_device = , op_namescope = /optimizer_50/, op_role = 2, op_role_var = ['mask_lm_trans_fc.w_0', 'elementwise_mul_50.tmp_0'])
    {ParamOut=['mask_lm_trans_layer_norm_bias']} = sgd(inputs={Grad=['elementwise_mul_51.tmp_0'], LearningRate=['learning_rate_0'], Param=['mask_lm_trans_layer_norm_bias']}, op_device = , op_namescope = /optimizer_51/, op_role = 2, op_role_var = ['mask_lm_trans_layer_norm_bias', 'elementwise_mul_51.tmp_0'])
    {ParamOut=['mask_lm_trans_layer_norm_scale']} = sgd(inputs={Grad=['elementwise_mul_52.tmp_0'], LearningRate=['learning_rate_0'], Param=['mask_lm_trans_layer_norm_scale']}, op_device = , op_namescope = /optimizer_52/, op_role = 2, op_role_var = ['mask_lm_trans_layer_norm_scale', 'elementwise_mul_52.tmp_0'])
    {ParamOut=['pos_embedding']} = sgd(inputs={Grad=['elementwise_mul_53.tmp_0'], LearningRate=['learning_rate_0'], Param=['pos_embedding']}, op_device = , op_namescope = /optimizer_53/, op_role = 2, op_role_var = ['pos_embedding', 'elementwise_mul_53.tmp_0'])
    {ParamOut=['pre_encoder_layer_norm_bias']} = sgd(inputs={Grad=['elementwise_mul_54.tmp_0'], LearningRate=['learning_rate_0'], Param=['pre_encoder_layer_norm_bias']}, op_device = , op_namescope = /optimizer_54/, op_role = 2, op_role_var = ['pre_encoder_layer_norm_bias', 'elementwise_mul_54.tmp_0'])
    {ParamOut=['pre_encoder_layer_norm_scale']} = sgd(inputs={Grad=['elementwise_mul_55.tmp_0'], LearningRate=['learning_rate_0'], Param=['pre_encoder_layer_norm_scale']}, op_device = , op_namescope = /optimizer_55/, op_role = 2, op_role_var = ['pre_encoder_layer_norm_scale', 'elementwise_mul_55.tmp_0'])
    {ParamOut=['sent_embedding']} = sgd(inputs={Grad=['elementwise_mul_56.tmp_0'], LearningRate=['learning_rate_0'], Param=['sent_embedding']}, op_device = , op_namescope = /optimizer_56/, op_role = 2, op_role_var = ['sent_embedding', 'elementwise_mul_56.tmp_0'])
    {ParamOut=['task_embedding']} = sgd(inputs={Grad=['elementwise_mul_57.tmp_0'], LearningRate=['learning_rate_0'], Param=['task_embedding']}, op_device = , op_namescope = /optimizer_57/, op_role = 2, op_role_var = ['task_embedding', 'elementwise_mul_57.tmp_0'])
    {ParamOut=['word_embedding']} = sgd(inputs={Grad=['elementwise_mul_58.tmp_0'], LearningRate=['learning_rate_0'], Param=['word_embedding']}, op_device = , op_namescope = /optimizer_58/, op_role = 2, op_role_var = ['word_embedding', 'elementwise_mul_58.tmp_0'])
}

